{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dba7c0d",
   "metadata": {},
   "source": [
    "# Домашнее задание № 10. Машинный перевод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yj7aripVIsbG",
   "metadata": {
    "id": "Yj7aripVIsbG"
   },
   "source": [
    "## Задание 1 (6 баллов + 2 доп балла).\n",
    "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). Используйте метрику BLEU. Найдите лучшие (как минимум 5) переводы согласно этой метрике и проверьте действительно ли они хорошие. Если все переводы нулевые, то пообучайте модель подольше.\n",
    "\n",
    "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Сейчас она работает только с одним текстом - это не эффективно. Можно генерировать переводы сразу для нескольких текстов (батча). Главная сложность с таким подходом состоит в том, что генерируемые тексты будут заканчиваться в разное время и нужно сделать столько итераций, сколько нужно для завершения всех текстов (т.е. условие на то, что последний токен не равен [END] в текущем коде не сработает). \n",
    "ВАЖНО - недостаточно просто изменить входной аргумент с text на texts и добавить еще один цикл по texts! Сама модель должна вызываться на нескольких текстах! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9ed858-f872-4bb0-9769-71a0ddca5b47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import keras\n",
    "import torch\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b7e138e-20b3-4b4b-8e96-23568bbf9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import keras\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce26de14-064f-4e8f-b27d-c5d305e19eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-04 09:19:11--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 67760131 (65M)\n",
      "Saving to: ‘data/MT/opus.en-ru-train.en’\n",
      "\n",
      "data/MT/opus.en-ru- 100%[===================>]  64.62M  37.6MB/s    in 1.7s    \n",
      "\n",
      "2024-04-04 09:19:13 (37.6 MB/s) - ‘data/MT/opus.en-ru-train.en’ saved [67760131/67760131]\n",
      "\n",
      "--2024-04-04 09:19:13--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 121340806 (116M)\n",
      "Saving to: ‘data/MT/opus.en-ru-train.ru’\n",
      "\n",
      "data/MT/opus.en-ru- 100%[===================>] 115.72M  37.9MB/s    in 3.1s    \n",
      "\n",
      "2024-04-04 09:19:16 (37.9 MB/s) - ‘data/MT/opus.en-ru-train.ru’ saved [121340806/121340806]\n",
      "\n",
      "--2024-04-04 09:19:16--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 173307 (169K)\n",
      "Saving to: ‘data/MT/opus.en-ru-test.en’\n",
      "\n",
      "data/MT/opus.en-ru- 100%[===================>] 169.25K   935KB/s    in 0.2s    \n",
      "\n",
      "2024-04-04 09:19:17 (935 KB/s) - ‘data/MT/opus.en-ru-test.en’ saved [173307/173307]\n",
      "\n",
      "--2024-04-04 09:19:17--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 305669 (299K)\n",
      "Saving to: ‘data/MT/opus.en-ru-test.ru’\n",
      "\n",
      "data/MT/opus.en-ru- 100%[===================>] 298.50K  1.15MB/s    in 0.3s    \n",
      "\n",
      "2024-04-04 09:19:17 (1.15 MB/s) - ‘data/MT/opus.en-ru-test.ru’ saved [305669/305669]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p data/MT\n",
    "!wget -O data/MT/opus.en-ru-train.en https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "!wget -O data/MT/opus.en-ru-train.ru https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "!wget -O data/MT/opus.en-ru-test.en https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
    "!wget -O data/MT/opus.en-ru-test.ru https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d22fc3e3-b9e6-4462-836f-c023dd0b57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('data/MT/opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
    "f = open('data/MT/opus.en-ru-train.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "872cd097-d975-4b54-a172-6d1ebf218b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('data/MT/opus.en-ru-test.ru').read().replace('\\xa0', ' ')\n",
    "f = open('data/MT/opus.en-ru-test.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8aa46409-31f6-4e27-9be1-7f9d7c8b571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = open('data/MT/opus.en-ru-train.en').read().lower().splitlines()\n",
    "ru_sents = open('data/MT/opus.en-ru-train.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d28ddd3d-a3ea-4cfe-9952-cd2928f0567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents_test = open('data/MT/opus.en-ru-test.en').read().lower().splitlines()\n",
    "ru_sents_test = open('data/MT/opus.en-ru-test.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "47c6e367-799c-42b6-ba0b-75472c8ce7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make uncased model\n",
    "\n",
    "tokenizer_en = Tokenizer(WordPiece(), )\n",
    "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_en = WordPieceTrainer(\n",
    "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "tokenizer_en.train(files=[f\"data/MT/opus.en-ru-{type}.en\" for type in [\"train\", \"test\"]], trainer=trainer_en )\n",
    "\n",
    "tokenizer_ru = Tokenizer(WordPiece(), )\n",
    "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_ru = WordPieceTrainer(\n",
    "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\", ], )\n",
    "tokenizer_ru.train(files=[f\"data/MT/opus.en-ru-{type}.ru\" for type in [\"train\", \"test\"]], trainer=trainer_ru )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8af6779-e12c-48ce-9f15-db501c84f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.decoder = decoders.WordPiece()\n",
    "tokenizer_ru.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9abfcb66-47c8-4a3a-87d1-0031382ef3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p tokenizers/MT\n",
    "tokenizer_en.save(\"tokenizers/MT/tokenizer_en.json\", pretty=True)\n",
    "tokenizer_ru.save(\"tokenizers/MT/tokenizer_ru.json\", pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5bfe581-744b-403c-a1ef-20955d094ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, target=False):\n",
    "    if target:\n",
    "        return [tokenizer.token_to_id('[START]')] + tokenizer.encode(text).ids + \\\n",
    "                [tokenizer.token_to_id('[END]')]\n",
    "    else:\n",
    "        return tokenizer.encode(text).ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11e6b576-278c-491e-be4e-26f26a4f71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, target=True) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d710fd98-dbb5-4943-aca9-1ba14ea9f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en_test = [encode(t, tokenizer_en) for t in en_sents_test]\n",
    "X_ru_test = [encode(t, tokenizer_ru, target=True) for t in ru_sents_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a6de16a-f2fa-4a9e-bbf9-dac2bbf8ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_en = np.max([len(x) for x in X_en])\n",
    "max_len_ru = np.max([len(x) for x in X_ru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4b109cc8-2a7c-4375-a49c-9cac32418fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17863, 19379)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_en, max_len_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce9190ac-0407-44f9-9d74-56d3a2d668e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_EN, MAX_LEN_RU = 33, 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc50f9f-7a55-447f-8ebe-e9d7bd0e45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = tokenizer_ru.token_to_id(\"[PAD]\")\n",
    "PAD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d233a404-f54c-42b4-bd1d-5900edd96dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts_en, texts_ru):\n",
    "        self.texts_en = [torch.LongTensor(sent)[:MAX_LEN_EN] for sent in texts_en]\n",
    "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, padding_value=PAD_ID, )\n",
    "        \n",
    "        self.texts_ru = [torch.LongTensor(sent)[:MAX_LEN_RU] for sent in texts_ru]\n",
    "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, padding_value=PAD_ID, )\n",
    "\n",
    "        self.length = len(texts_en)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ids_en = self.texts_en[:, index]\n",
    "        ids_ru = self.texts_ru[:, index]\n",
    "\n",
    "        return ids_en, ids_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "956df013-aaac-439c-98c4-6a649b84d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(X_en, X_ru)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=128, shuffle=True, )\n",
    "\n",
    "valid_set = Dataset(X_en_test, X_ru_test)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "353c3444-91ec-4f8e-aa86-89d39a8dbe1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f113a51d-e9f5-42d6-a687-344b1be4f26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e9a21d79-f32a-4905-a059-c91206b5e113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c45823-b529-44a9-9ce7-ad01b4184565",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57d9fba-3bb4-4c76-84d9-7faeadee7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 150):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=emb_size, \n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: torch.Tensor,\n",
    "                trg: torch.Tensor,\n",
    "                src_mask: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor,\n",
    "                src_padding_mask: torch.Tensor,\n",
    "                tgt_padding_mask: torch.Tensor,\n",
    "                memory_key_padding_mask: torch.Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "        x = self.generator(outs)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory, tgt_mask)\n",
    "        \n",
    "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
    "def generate_square_subsequent_mask(size):\n",
    "    mask = (torch.triu(torch.ones((size, size), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_ID).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_ID).transpose(0, 1)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "b94af205-3eef-4fc1-8c1a-a65220ea3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, print_every=500):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    ac = []\n",
    "    \n",
    "    model.train()  \n",
    "\n",
    "    for i, (texts_en, texts_ru) in tqdm(enumerate(iterator)):\n",
    "        texts_en = texts_en.T.to(DEVICE) # чтобы батч был в конце\n",
    "        texts_ru = texts_ru.T.to(DEVICE) # чтобы батч был в конце\n",
    "        \n",
    "        # помимо текста в модель еще нужно передать целевую последовательность\n",
    "        # но не полную а без 1 последнего элемента\n",
    "        # а на выходе ожидаем, что модель сгенерирует этот недостающий элемент\n",
    "        texts_ru_input = texts_ru[:-1, :]\n",
    "        \n",
    "        \n",
    "        # в трансформерах нет циклов как в лстм \n",
    "        # каждый элемент связан с каждым через аттеншен\n",
    "        # чтобы имитировать последовательную обработку\n",
    "        # и чтобы не считать аттеншн с паддингом \n",
    "        # в трансформерах нужно считать много масок\n",
    "        # подробнее про это по ссылкам выше\n",
    "        (texts_en_mask, texts_ru_mask, \n",
    "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
    "        logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
    "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # сравниваем выход из модели с целевой последовательностью уже с этим последним элементом\n",
    "        texts_ru_out = texts_ru[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {np.mean(epoch_loss)};')\n",
    "        \n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_f1 = []\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for i, (texts_en, texts_ru) in tqdm(enumerate(iterator)):\n",
    "            texts_en = texts_en.T.to(DEVICE)\n",
    "            texts_ru = texts_ru.T.to(DEVICE)\n",
    "\n",
    "            texts_ru_input = texts_ru[:-1, :]\n",
    "\n",
    "            (texts_en_mask, texts_ru_mask, \n",
    "            texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
    "\n",
    "            logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
    "                           texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "\n",
    "            \n",
    "            texts_ru_out = texts_ru[1:, :]\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7677dba7-b8f8-4a9c-9c05-6dff01961739",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17)\n",
    "\n",
    "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
    "RU_VOCAB_SIZE = tokenizer_ru.get_vocab_size()\n",
    "\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, EN_VOCAB_SIZE, RU_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_ID).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), \n",
    "                              lr=3e-4, \n",
    "                              betas=(0.9, 0.98), \n",
    "                              eps=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "29fea54f-a656-4778-8c4b-8d024a399925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p models/MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ef568a19-4f76-4012-961e-922a2209829e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 2.640, Val loss: 2.723,            Epoch time=1952.652s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    \n",
    "    start_time = timer()\n",
    "    train_loss = train(transformer, training_generator, optimizer, loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer, valid_generator, loss_fn)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(transformer, 'models/MT/model.ckpt')\n",
    "    \n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(transformer, 'models/MT/model.ckpt')\n",
    "    \n",
    "    losses.append(val_loss)\n",
    "        \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1b1b39-bbdf-4315-b109-4ea7934ffb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6286fd5d-0124-4287-a96d-fe7bac914efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (generator): Linear(in_features=256, out_features=30000, bias=True)\n",
       "  (src_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(30000, 256)\n",
       "  )\n",
       "  (tgt_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(30000, 256)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05dc94-93ba-4ebd-b3cc-144f53b849af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f159085-cd8e-48f9-945d-3062d8f7a799",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e397b-607b-408a-aecc-2e8ce0f9bfed",
   "metadata": {},
   "source": [
    "##### Base implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02924757-517e-4305-b3e5-2070eff81c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "\n",
    "\n",
    "    input_ids = tokenizer_en.encode(text).ids[:MAX_LEN_EN]\n",
    "    output_ids = [tokenizer_ru.token_to_id('[START]')]\n",
    "\n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)]).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
    "\n",
    "    (texts_en_mask, texts_ru_mask, \n",
    "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                   texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "    pred = logits.argmax(2).item()\n",
    "\n",
    "    while pred not in [tokenizer_ru.token_to_id('[PAD]'), tokenizer_ru.token_to_id('[END]')] and output_ids_pad.size()[0] < 150:\n",
    "        output_ids.append(pred)\n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
    "\n",
    "        (texts_en_mask, texts_ru_mask, \n",
    "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "\n",
    "        pred = logits.argmax(2)[-1].item()\n",
    "\n",
    "    return tokenizer_ru.decode(output_ids, skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af447b-bbb1-477c-b8c4-bc10687a9c26",
   "metadata": {},
   "source": [
    "##### Efficient implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c9f0a-0d08-49b9-b60a-1a95894deb0f",
   "metadata": {},
   "source": [
    "Note: we could have spawned an independent process for the needed batch length with their independent model instances (on different gpus / cpu cores) and solve the task this way -- however it seems like a massive overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7385bcd-7237-4eb1-8ad6-5eec220964d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f50c687a-255d-4886-b60f-9b01ece1a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torch.load(\"models/MT/model.ckpt\", )\n",
    "tokenizer_en = PreTrainedTokenizerFast(tokenizer_file=\"tokenizers/MT/tokenizer_en.json\")\n",
    "tokenizer_ru = PreTrainedTokenizerFast(tokenizer_file=\"tokenizers/MT/tokenizer_ru.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2421ded8-c7de-4a62-9089-4023dd95df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents_test = open('data/MT/opus.en-ru-test.en').read().lower().splitlines()\n",
    "ru_sents_test = open('data/MT/opus.en-ru-test.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e28661b4-d9f1-4860-8b6d-061da2340ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a5b44481-49ed-49dc-bc0c-53883a58ce81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[START]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[END]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7859acd-96d7-4dcf-9a16-f161aa11a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f4025e4-0276-40dd-976c-505a744090a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, target=False):\n",
    "    if target:\n",
    "        return [2] + tokenizer.encode(text) + [3]\n",
    "    else:\n",
    "        return tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "362fa933-fb59-4a8a-93d1-5b2675503cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_en_test = [encode(t, tokenizer_en) for t in en_sents_test]\n",
    "X_ru_test = [encode(t, tokenizer_ru, target=True) for t in ru_sents_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "88c496de-d4c3-400e-b94b-d3d4c39fc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = Dataset(X_en_test, X_ru_test)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "14720d93-f271-44ff-a5c6-55b528ec2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(text_batch):\n",
    "\n",
    "    input_ids = [tokenizer_en.encode(text)[:MAX_LEN_EN] for text in text_batch]\n",
    "    output_ids = [tokenizer_ru.encode('[START]') for text in text_batch]\n",
    "\n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_id) for input_id in input_ids]).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_id) for output_id in output_ids]).to(DEVICE)\n",
    "    output_ids = torch.Tensor(output_ids)\n",
    "\n",
    "    (texts_en_mask, texts_ru_mask, \n",
    "    texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                     texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "    \n",
    "    pred = logits.argmax(2).T.to(\"cpu\")\n",
    "    output_ids = torch.cat((output_ids, pred), axis = 1).long()\n",
    "    \n",
    "    while any([x.tolist() not in [tokenizer_ru.encode('[PAD]'), tokenizer_ru.encode('[END]')] for x in pred]) \\\n",
    "            and output_ids_pad.size()[0] < 150:\n",
    "         \n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence(output_ids).to(DEVICE)\n",
    "\n",
    "        (texts_en_mask, texts_ru_mask, \n",
    "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "        \n",
    "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                        texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "\n",
    "        pred = logits.argmax(2).T.to(\"cpu\")[:, -1:]\n",
    "        # if pred[-1] == \"[END]\" -> change pred to pad\n",
    "        bool_mask = (output_ids[:, -1:] == 3) | (output_ids[:, -1:] == 1)\n",
    "        pred[:, -1:][bool_mask] = 1\n",
    "        \n",
    "        output_ids = torch.cat((output_ids, pred), axis = 1).long()\n",
    "    \n",
    "    return [tokenizer_ru.decode(output_id, skip_special_tokens=True) for output_id in output_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2eca8842-6a36-4146-8630-00220a610c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['если вы хотите увидеть это через 10 лет, вам нужно восемь до 10 лет, если только не захотите рисковать пауни становится...',\n",
       " 'люксембургский форум по теме « won group », москва, 14 апреля 2008 @@ w @@ w @@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @# w @#@ w @#@ w @# w @# w @# w @# w @# w @#@ w @#@ w @# w @ w @ w @ won @ woncow @ w @ w @ w @ w @',\n",
       " 'эта задача должна быть предоставлена возможность установить руководящую группу, включающей представителей контакта. @@ w @#*#@ w @#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#',\n",
       " 'но что можно сделать по этому? @@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @#@ w @# w @ w @#@ w @#@ w @ w @ w @ w @ w @ w @ w @ w @ w @ w @ w @ w @ won @ won @ won @ won @ w @ won @ w @ w @ won @ won# won# won @ won @ won @ won @ won @ won @ won @ w',\n",
       " 'когда я получу винтовки из дверимана, ты знаешь, как повезло, что ты будешь?']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_batch(en_sents_test[35:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9bdf9-2eeb-4571-8928-24cdc2a8f802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5e153939-e1da-4248-95c6-69efa847434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_text = \"This is a target sample about a brown fox jumping over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "8ff2e8db-d47e-45bb-a8d9-6018c17b7657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'можешь перевести это предложение?'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"can you translate this sentence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "54a08e92-a33f-4f26-b8cd-0950e2c03866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'предложение в русском языке'"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"sentence into russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c8fd65ef-5178-428f-83f1-849644c2f0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'это мишень о коричневой лисе, подмеивший собакой собакой.'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(eng_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b8f62-792b-4ae6-a761-9e4d3a62e759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc179d7c-ea9c-4724-aade-89c1578004fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c873756a-6702-4ecc-8e77-4c0124f8840d",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "210cda5f-d883-4d98-8e4a-58066802c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3473760d-fc76-430e-9c0d-be0c3736fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:23<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "source": [
    "translations = []\n",
    "\n",
    "for text in tqdm(en_sents_test):\n",
    "    translations.append(translate(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "043a02f5-2a62-4ba6-97d4-9f340970e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/local/.local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/local/.local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/local/.local/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "2000it [00:00, 3652.04it/s]\n"
     ]
    }
   ],
   "source": [
    "bleus = []\n",
    "\n",
    "for i, t in tqdm(enumerate(translations)):\n",
    "    reference = tokenizer_ru.encode(t).tokens\n",
    "    hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
    "\n",
    "    bleus.append(round(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  auto_reweigh=True), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "dabe62cc-b390-49bf-b011-ec3aae8d54c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.575249999999997"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(bleus)/len(bleus))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "e7857007-4513-47de-a057-e1be72133a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_translations = np.argsort(np.array(bleus))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "2a0e5a9c-46a1-46cb-a312-c20be5d97c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yeah.', 'да.', 'да.'),\n",
       " ('#4 長老議會 25h, wipe (00:02:05)',\n",
       "  '#4 совет старейшин 25 гер., вайп (00:02:05)',\n",
       "  '# 4 совет старейшин 25 гер., вайп ( 00 : 02 : 05 )'),\n",
       " ('cincinnati (ohio) 513231 **** mobile',\n",
       "  'цинциннати (огайо) 513231 **** мобильный',\n",
       "  'цинциннати ( огайо ) 513231 **** мобильный'),\n",
       " ('shahir...', 'шахир...', 'шахир...'),\n",
       " ('victoria (virginia) 434696 **** phone',\n",
       "  'виктория (вирджиния) 434696 **** телефон',\n",
       "  'виктория ( вирджиния ) 434696 **** телефон'),\n",
       " ('https://www.forumhouse.ru/forums/31/',\n",
       "  'https://www.forumhouse.ru/forums/31/',\n",
       "  'https :// www. forumhouse. ru / forums / 31 /'),\n",
       " ('- vivian wilkes.', '-вивиан уилкс.', '- вивиан уилкс.'),\n",
       " ('jon: what is this?', 'что это?', 'что это?'),\n",
       " (\"how's the internet doing without me? it's ivan\",\n",
       "  'как там интернет без меня?',\n",
       "  'как там интернет без меня?'),\n",
       " ('4,907usd', '4,907usd', '4, 907usd'),\n",
       " ('pepper is a gift.', 'пеппер - это подарок.', 'пеппер - это подарок.'),\n",
       " ('tina, what are you doing?',\n",
       "  'тина, что ты делаешь?',\n",
       "  'тина, что ты делаешь?'),\n",
       " (\"he doesn't move...\", 'он не двигается...', 'он не двигается...'),\n",
       " ('finally, you can now discover everything you need to know about your fax file... instantly!',\n",
       "  'теперь можно мгновенно получить всю необходимую информацию о файле fax!',\n",
       "  'теперь можно мгновенно получить всю необходимую информацию о файле fax!'),\n",
       " ('yeah.', 'да.', 'да.'),\n",
       " ('documentation: ece/trans/sc.3/wp.3/2008/16/rev.1',\n",
       "  'документация: ece/trans/sc.3/wp.3/2008/16/rev.1',\n",
       "  'документация : ece / trans / sc. 3 / wp. 3 / 2008 / 16 / rev. 1'),\n",
       " ('s5000', 's5000', 's5000'),\n",
       " ('decides to include in the provisional agenda of its sixtieth session the item entitled “financing of the united nations mission in liberia”.',\n",
       "  'постановляет включить в предварительную повестку дня своей шестидесятой сессии пункт, озаглавленный «финансирование миссии организации объединенных наций в либерии».',\n",
       "  'постановляет включить в предварительную повестку дня своей шестидесятой сессии пункт, озаглавленный « финансирование миссии организации объединенных наций в либерии ».'),\n",
       " ('recalling also the relevant security council resolutions, including resolutions 242\\xa0(1967) of 22\\xa0november 1967 and 338\\xa0(1973) of 22\\xa0october 1973,',\n",
       "  'ссылаясь также на соответствующие резолюции совета безопасности, включая резолюции 242 (1967) от 22 ноября 1967 года и 338 (1973) от 22 октября 1973 года,',\n",
       "  'ссылаясь также на соответствующие резолюции совета безопасности, включая резолюции 242 ( 1967 ) от 22 ноября 1967 года и 338 ( 1973 ) от 22 октября 1973 года,'),\n",
       " ('thanks.', 'спасибо.', 'спасибо.'),\n",
       " ('yeah.', 'да.', 'да.'),\n",
       " (\"it's not a red door.\", 'это не красная дверь.', 'это не красная дверь.'),\n",
       " ('припылить', 'припылить', 'припылить'),\n",
       " ('nice work, moz.', 'отличная работа, моз.', 'отличная работа, моз.'),\n",
       " ('thu, 01/14/2010 - 01:29',\n",
       "  'чт, 01/14/2010 - 01:29',\n",
       "  'чт, 01 / 14 / 2010 - 01 : 29'),\n",
       " ('we talked to davis.', 'мы говорили с дэвисом.', 'мы говорили с дэвисом.'),\n",
       " ('öåíà: 12800äîë.', 'öåíà: 12800äîë.', 'öåíà : 12800äîë.'),\n",
       " ('siberian federal university',\n",
       "  'сибирский федеральный университет',\n",
       "  'сибирский федеральный университет'),\n",
       " ('of all the students in the country, he chose our miss franklin.',\n",
       "  'из всех студентов в стране он выбрал нашу мисс франклин.',\n",
       "  'из всех студентов в стране он выбрал нашу мисс франклин.'),\n",
       " ('-it ruined our marriage.',\n",
       "  '- это разрушило наш брак.',\n",
       "  '- это разрушило наш брак.'),\n",
       " ('ночи:', 'ночи:', 'ночи :'),\n",
       " ('yeah.', 'да.', 'да.'),\n",
       " ('<<< november 2666 >>>', '<<< ноябрь 2666 >>>', '<<< ноябрь 2666 >>>'),\n",
       " ('published: 24 october 2014',\n",
       "  'опубликовано: 24 октябрь 2014',\n",
       "  'опубликовано : 24 октябрь 2014'),\n",
       " ('98799', '98799', '98799'),\n",
       " ('vacations are a disaster.',\n",
       "  'отпуск - это катастрофа.',\n",
       "  'отпуск - это катастрофа.'),\n",
       " ('trans/wp.5/2000/4/add.1/rev.1.',\n",
       "  'trans/wp.5/2000/4/add.1/rev.1.',\n",
       "  'trans / wp. 5 / 2000 / 4 / add. 1 / rev. 1.'),\n",
       " ('it was... the 14th of july...',\n",
       "  'это было... 14 июля...',\n",
       "  'это было... 14 июля...'),\n",
       " ('resolution 58/245', 'резолюция 58/245', 'резолюция 58 / 245'),\n",
       " ('geneva, 7-11 march 2005',\n",
       "  'женева, 7-11 марта 2005 года',\n",
       "  'женева, 7 - 11 марта 2005 года'),\n",
       " ('19. 2. 2016', '19. 2. 2016', '19. 2. 2016'),\n",
       " ('yeah.', 'да.', 'да.'),\n",
       " ('good night.', 'спокойной ночи.', 'спокойной ночи.'),\n",
       " ('11.05.15 06:05', '11.05.15 06:05', '11. 05. 15 06 : 05'),\n",
       " ('chloe: jack, this is crazy.', 'джек, это безумие.', 'джек, это безумие.'),\n",
       " ('#14 tortos 10h, wipe (00:04:49)',\n",
       "  '#14 тортос 10 гер., вайп (00:04:49)',\n",
       "  '# 14 тортос 10 гер., вайп ( 00 : 04 : 49 )'),\n",
       " (\"but it's not an easy process.\",\n",
       "  'но это не простой процесс.',\n",
       "  'но это не простой процесс.'),\n",
       " ('yes.', 'да.', 'да.'),\n",
       " ('yes.', 'да.', 'да.'),\n",
       " ('letter dated 19 december 2003 from the chairman of the security council committee established pursuant to resolution 1373 (2001) concerning counter-terrorism addressed to the president of the security council',\n",
       "  'письмо председателя комитета совета безопасности, учрежденного резолюцией 1373 (2001) о борьбе с терроризмом, от 19 декабря 2003 года на имя председателя совета безопасности',\n",
       "  'письмо председателя комитета совета безопасности, учрежденного резолюцией 1373 ( 2001 ) о борьбе с терроризмом, от 19 декабря 2003 года на имя председателя совета безопасности'),\n",
       " ('(6-8 september 2000)',\n",
       "  '(6-8 сентября 2000 года)',\n",
       "  '( 6 - 8 сентября 2000 года )'),\n",
       " ('paragraphs\\xa04.1.1. and 4.1.2., amend to read:',\n",
       "  'пункты 4.1.1 и 4.1.2 изменить следующим образом:',\n",
       "  'пункты 4. 1. 1 и 4. 1. 2 изменить следующим образом :'),\n",
       " ('i need to talk to him about his films.',\n",
       "  'мне нужно поговорить с ним о его фильмах.',\n",
       "  'мне нужно поговорить с ним о его фильмах.'),\n",
       " (\"that's not the uterus.\", 'это не матка.', 'это не матка.'),\n",
       " ('ray, door!', 'рэй, дверь!', 'рэй, дверь!'),\n",
       " ('lottery numbers: 2 , 11 , 12 , 19 , 26 , 41',\n",
       "  'лотерея номера: 2, 11, 12, 19, 26, 41',\n",
       "  'лотерея номера : 2, 11, 12, 19, 26, 41'),\n",
       " ('дата публикации: 16 октября 2014 г.',\n",
       "  'дата публикации: 16 октября 2014 г.',\n",
       "  'дата публикации : 16 октября 2014 г.'),\n",
       " ('you are the8.873.184th visitor since 31 march 2001',\n",
       "  'ты8.873.184го посетителя, так как 31 марта 2001 года',\n",
       "  'ты8. 873. 184го посетителя, так как 31 марта 2001 года'),\n",
       " ('12.02.', '12.02.', '12. 02.'),\n",
       " ('fax: (+374 1) 538 428',\n",
       "  'факс: (+374 1) 538 428',\n",
       "  'факс : (+ 374 1 ) 538 428'),\n",
       " ('lohengrin? .', 'лоэнгрин?', 'лоэнгрин?'),\n",
       " ('204.155.146.171', '204.155.146.171', '204. 155. 146. 171'),\n",
       " ('spain ( 15 apartments) apartments',\n",
       "  'испания ( 15 апартаментов) апартаментов',\n",
       "  'испания ( 15 апартаментов ) апартаментов'),\n",
       " ('www.monster.ie', 'www.monster.ie', 'www. monster. ie'),\n",
       " ('yeah.', 'да.', 'да.'),\n",
       " ('dvb-s 1/2, 2/3, 3/4, 5/6, 7/8 fec',\n",
       "  'dvb-s 1/2, 2/3, 3/4, 5/6, 7/8 fec',\n",
       "  'dvb - s 1 / 2, 2 / 3, 3 / 4, 5 / 6, 7 / 8 фек'),\n",
       " ('hotel, restaurant 75 rooms overlooking lake kivu.',\n",
       "  'гостиница, ресторан 75 номеров с видом на озеро киву.',\n",
       "  'отель, ресторан 75 номеров с видом на озеро киву.'),\n",
       " ('sat, 03/15/2014 - 22:03',\n",
       "  'сб, 03/15/2014 - 22:03',\n",
       "  'sat, 03 / 15 / 2014 - 22 : 03'),\n",
       " ('http://www.culturalnet.ru/f/viewtopic.php?id=1305',\n",
       "  'http://www.culturalnet.ru/f/viewtopic.php?id=1611',\n",
       "  'http :// www. culturalnet. ru / f / viewtopic. php? id = 1305'),\n",
       " ('(b) identifying, developing and strengthening mechanisms for the transfer of technology in the field of environmentally sound management of hazardous wastes or their minimization in the region;',\n",
       "  'b) выявление, разработка и укрепление механизмов передачи технологии в области экологически обоснованного регулирования опасных отходов или их минимизации в регионе;',\n",
       "  'b ) выявление, разработка и укрепление механизмов передачи технологий в области экологически обоснованного регулирования опасных отходов или их минимизации в регионе ;'),\n",
       " (\"i can't remember why i'm here. (coughs)\",\n",
       "  'не могу вспомнить, почему я здесь.',\n",
       "  'я не могу вспомнить, почему я здесь.'),\n",
       " ('amber, jasmine, jade.', 'амбер, жасмин, джейд.', 'эмбер, жасмин, джейд.'),\n",
       " ('click to see the amount of minerals in fruits, vegetables, fats, cereals, spices and nuts rich in vitamin b2',\n",
       "  'click to see the amount of minerals in fruits, vegetables, fats, cereals, spices and nuts rich in phosphorus',\n",
       "  'click to see the amount of minerals in fruits, vegetables, fats, cereals, spices and nuts rich in phdin'),\n",
       " ('the chairman: the committee will now take action on the draft resolution, as orally revised.',\n",
       "  'председатель (говорит по-английски): сейчас комитет примет решение по проекту резолюции с внесенными в него устными поправками.',\n",
       "  'председатель ( говорит по - английски ): сейчас комитет примет меры по проекту резолюции с внесенными в него устными поправками.'),\n",
       " ('can i be a dispatcher?',\n",
       "  'а я могу быть диспетчером?',\n",
       "  'я могу быть диспетчером?'),\n",
       " (\"cntrtextmig.dll device can't work/no sound: actually, in most cases device problem is because of .sys files.\",\n",
       "  'osbaseln.dll устройство не может работать/нет звука: на самом деле, в большинстве случаев, устройство проблема в том, что из системы файлов.',\n",
       "  'cntrabnce. dll устройство не может работать / нет звука : на самом деле, в большинстве случаев, устройство проблема в том, что из системы файлов.'),\n",
       " ('wednesday, 19.04.2023', 'wednesday, 19.04.2023', 'среда, 19. 04. 2023'),\n",
       " ('gross area 174.00 m²', 'общая площадь 174.00 m²', 'площадь 174. 00 m ²'),\n",
       " ('the united nations disarmament yearbook, vol. 18: 1993 (united nations publication, sales no. e.94.a.1), appendix ii.',\n",
       "  'ежегодник организации объединенных наций по разоружению, том 18: 1993 год (издание организации объединенных наций, в продаже под № r.94.ix.1), добавление ii.',\n",
       "  'ежегодник организации объединенных наций по разоружению, том 18 : 1993 ( издание организации объединенных наций, в продаже под № r. 94. a. 1 ), добавление ii.'),\n",
       " ('letter dated 13 august 2004 from the secretary-general of the north atlantic treaty organization addressed to the secretary-general',\n",
       "  'письмо генерального секретаря организации североатлантического договора от 13 августа 2004 года на имя генерального секретаря',\n",
       "  'письмо генерального секретаря североатлантического договора от 13 августа 2004 года на имя генерального секретаря'),\n",
       " ('bonita springs (florida) 239992 **** phone',\n",
       "  'бонита спрингс (флорида) 239992 **** телефон',\n",
       "  'бонта спрингс ( флорида ) 239992 **** телефон'),\n",
       " (\"i don't know,dear.\", 'не знаю, дорогая.', 'я не знаю, дорогая.'),\n",
       " ('alphonse capone.', 'альфонс капоне', 'альфонс капоне.'),\n",
       " (\"you're lucky that i convinced him not to use shrimp.\",\n",
       "  'тебе повезло, что я убедила его не использовать креветки.',\n",
       "  'тебе повезло, что я убедил его не использовать креветки.'),\n",
       " ('#12 sha of fear 25, wipe (00:03:46)',\n",
       "  '#12 ша страха 25, вайп (00:03:46)',\n",
       "  '# 12 ша гордыни 25, вайп ( 00 : 03 : 46 )'),\n",
       " ('the list of international forums to be consulted is contained in ece/mp.pp/wg.1/2006/3/add.2.',\n",
       "  'перечень международных форумов для проведения консультаций проводится в документе ece/mp.pp/wg.1/2006/3/add.2.',\n",
       "  'перечень международных форумов, которые содержатся в документе ece / mp. pp / wg. 1 / 2006 / 3 / add. 2.'),\n",
       " ('wait a minute, harry.', 'минутку, гарри.', 'подождите минутку, гарри.'),\n",
       " ('our friend over in brussels?',\n",
       "  'наш друг в брюсселе',\n",
       "  'наш друг в брюсселе?'),\n",
       " ('lyuberetskiy district (moscow region) 4952607 *** phone',\n",
       "  'люберецкий район (московская область) 4952607 *** телефон',\n",
       "  'лицкий район ( московская область ) 4952607 *** телефон'),\n",
       " ('1 2 3 4 5 6 7 8 [next>>]',\n",
       "  '1 2 3 4 5 6 7 8 [след>>]',\n",
       "  '1 2 3 4 5 6 7 8 [ next >>]'),\n",
       " ('the working group approved the substance of draft article 15 and referred it to the drafting group.',\n",
       "  'рабочая группа одобрила содержание проекта статьи 15 и передала его на рассмотрение редакционной группе.',\n",
       "  'рабочая группа одобрила содержание проекта статьи 15 и передала его редакционной группе.'),\n",
       " ('activex, com, com+ (dcom), corba',\n",
       "  'activex, com, com+ (dcom), corba',\n",
       "  'activex, com, + ( dcom ), corba'),\n",
       " ('resolution 1321 (2000)',\n",
       "  'резолюция 1321 (2000),',\n",
       "  'резолюция 1321 ( 2000 )'),\n",
       " ('second session of the working group 4 - 9 4',\n",
       "  'а. вторая сессия рабочей группы 4 - 9 4',\n",
       "  'вторая сессия рабочей группы 4 - 9 4'),\n",
       " ('this is agent reid from the fbi.',\n",
       "  'это агент рид из фбр.',\n",
       "  'агент рид из фбр.'),\n",
       " ('august 23, 2008, ps3 newz, ps3 gamez',\n",
       "  'суббота, 23 августа, 2008',\n",
       "  'среда, 23 августа, 2008'),\n",
       " ('look at him, michael.',\n",
       "  'смотри на него, майкл.',\n",
       "  'посмотри на него, майкл.'),\n",
       " ('- elsa and anna?', '- эльза и анна?', 'эльза и анна?'),\n",
       " (\"'cause they're women.\",\n",
       "  'ну потому что они женщины.',\n",
       "  'потому что они женщины.'),\n",
       " (\"christian -- where's he -- he's fine.\",\n",
       "  'кристиан... где он... он в порядке.',\n",
       "  'христианина... где он... он в порядке.')]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source text, target text, translation\n",
    "\n",
    "list(zip(\n",
    "    np.array(en_sents_test)[best_translations[:100]], \n",
    "    np.array(ru_sents_test)[best_translations[:100]], \n",
    "    np.array(translations)[best_translations[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d262b7-f6e2-4594-9b24-0f24854df13a",
   "metadata": {},
   "source": [
    "Вообще, результаты чуть-чуть выглядят как data leak из теста в трейн -- все же идеально переводятся крайне специфичные примеры. Хотя возможно это какой-то мой собственный bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa93d6",
   "metadata": {},
   "source": [
    "\n",
    "## Задание 2 (2 балла).\n",
    "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
    "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как ее применить к паре en->ru на данных из семинара. Сколько моделей понадобится? Сколько запусков обучения нужно будет сделать?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961b405-31e8-4231-87cf-fa0b7dab0e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32c01929-5a5e-456e-a44e-192cdb3fa317",
   "metadata": {},
   "source": [
    "Back translation -- подход, позволяющий создать параллельный корпус для обучения модели машинного перевода автоматическими методами без привлечения носителей для пополнения датасета/разметки. Тем не менее, подход предполагает наличие хотя бы ограниченно репрезентативного параллельного корпуса, позволяющего моделировать уровни языка (хотя бы семантику/морфологию и ограниченно синтаксис) для построения изначальной модели. \n",
    "Также важным условием является наличие большего по объему моноязычного корпуса на одном из языков.\n",
    "\n",
    "Идея заключается в том, чтобы сначала обучить модель для перевода с языка, где доступен большой моноязыковой корпус, а после использовать её для расширения обучающей выборки паралельного корпуса, и последующего обучения модели обратного перевода (с языка, где большой моноязычный корпус отсутствует на один из крупных языков).\n",
    "\n",
    "Кажется важным отметить, что по идее такой метод должен быть весьма чувствителен к чересчур однообразным параллельным текстам, а также может страдать от недостаточности размера словаря малого языка, или доменной специфичности текстов, присутствовавших в параллельном корпусе (что довольно характерно для малых языков), что скажется на обобщающей способности первоначальной модели перевода.\n",
    "\n",
    "Итого, две модели: \n",
    "1. target-to-source,\n",
    "2. source-to-target\n",
    "\n",
    "и для запуска обучения соотвтственно; остальное -- генерация и вопросы оценки состоятельности модели, отсутсвия дрифтов в данных"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
