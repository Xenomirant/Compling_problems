{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14be5b3",
   "metadata": {},
   "source": [
    "## Задание 1. (10 баллов)\n",
    "\n",
    "Дообучите языковую модель на датасете инструкций, используя LoRA. Проверьте, что дообученная модель отличается от изначальной - сгенерируйте продолжения для одних и тех же промптов и сравните результаты.\n",
    "\n",
    "Вы можете взять за основу код семинара PEFT, изменив датасет цитат на датасет инструкций (можно просто скопировать из семинара про General_instruct_fine-tuning). \n",
    "Можно использовать alpaca_dataset, датасет Dolly 2 или переведенный датасет (или все вместе). \n",
    "Важно использовать модель с большим количеством параметров (относительно семинара по General instruct fine-tuning). \n",
    "Размер модели должен быть как минимум 3 млрд параметров.  \n",
    "**Нужно использовать модель, которую мы не разбирали на семинаре (OPT-2.7b, OPT-6.7b). Найдите новую модель на huggingface hub.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a624a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!  pip install hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecc25f-8d4c-4152-b4bb-29c12629edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df8580-d5d7-457a-bdd5-982dd2aafbea",
   "metadata": {},
   "source": [
    "С моей GPU дефолтная квантизация в bitsandbytes не поддерживается, так что либо  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b976af-8d8b-4faa-996d-0a4c123e9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer\n",
    "from hqq.models.hf.llama import LlamaHQQ\n",
    "from datasets import load_dataset\n",
    "import torch, time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hqq.core.quantize import *\n",
    "from hqq.core.peft import PeftUtils\n",
    "from hqq.core.quantize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbcd02f8-308b-4c85-aa20-da35ef3678ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model and setttings\n",
    "model_id      = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "compute_dtype = torch.float32\n",
    "device        = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ef31b8-fe31-4852-a44e-5da39dcfd7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_instruction(instruct, text, model):  \n",
    "\n",
    "    inputs = tokenizer([instruct.format(text)], \n",
    "                        return_tensors=\"pt\",).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(\"cuda\"):\n",
    "        \n",
    "            output_sequences = model.generate(\n",
    "                # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "                num_beams=5,\n",
    "                max_length=1024,\n",
    "            # no_repeat_ngram_size=3, \n",
    "            repetition_penalty= 3.0,\n",
    "            length_penalty=0.01,\n",
    "            do_sample=True, \n",
    "            temperature=1.5,\n",
    "            # top_k=15, \n",
    "            # top_p=0.8, \n",
    "            early_stopping=True,\n",
    "            num_return_sequences=3,\n",
    "            # num_return_sequences=1,\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            )\n",
    "    summaries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04708bd-512a-44a6-9db5-509e7d339c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "\ttorch.cuda.empty_cache()\n",
    "\tgc.collect()\n",
    "\n",
    "\n",
    "def eval_wikitext2(model, tokenizer, max_length=1024, stride=512, verbose=True):\n",
    "\n",
    "    model.eval()\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.add_eos_token = False\n",
    "\n",
    "    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    encodings = tokenizer('\\n\\n'.join(dataset['text']), return_tensors='pt')\n",
    "\n",
    "    encodings['input_ids'] = encodings['input_ids'].to('cuda')\n",
    "\n",
    "    lls, t = [], []\n",
    "    for i in tqdm(range(0, encodings['input_ids'].size(1), stride), disable=not verbose):\n",
    "        begin_loc  = max(i + stride - max_length, 0)\n",
    "        end_loc    = min(i + stride, encodings['input_ids'].size(1))\n",
    "        trg_len    = end_loc - i  \n",
    "        input_ids  = encodings['input_ids'][:,begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:,:-trg_len] = -100 #ignore context \n",
    "\n",
    "        t1 = time.time()\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                log_likelihood = model(input_ids, labels=target_ids).loss * trg_len\n",
    "        torch.cuda.synchronize()\n",
    "        t2 = time.time()\n",
    "        t.append((t2-t1))\n",
    "        lls.append(log_likelihood)\n",
    "\n",
    "        del input_ids, target_ids\n",
    "\n",
    "    ppl = np.round(float(torch.exp(torch.stack(lls).sum() / end_loc)), 4)\n",
    "    pred_time = np.round(np.mean(t), 3)\n",
    "    if(verbose):\n",
    "        print('perplexity', ppl)\n",
    "        print('time', str(pred_time) + '  sec')\n",
    "\n",
    "    del encodings\n",
    "    cleanup()\n",
    "\n",
    "    return {'perplexity':ppl, 'prediction_time':pred_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "671f845a-4b1a-4f3e-9e55-552b58477a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ac969cb02b446780afbb0c2bb5c141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 1067.07it/s]\n",
      "100%|██████████| 32/32 [00:56<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "#Load model on the CPU\n",
    "######################\n",
    "model     = HQQModelForCausalLM.from_pretrained(model_id, torch_dtype=compute_dtype, cache_dir=\"./models\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "#Quantize the model\n",
    "######################\n",
    "\n",
    "quant_config = BaseQuantizeConfig(nbits=2, group_size=8, \n",
    "                                  offload_meta=True,)\n",
    "model.quantize_model(quant_config=quant_config, compute_dtype=compute_dtype, device=device) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a808366-3c7d-4f12-8878-f8f8414c84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./models/quantized/llama-2-7b-2bit_8-group\"\n",
    "\n",
    "#Save the quantized model\n",
    "model.save_quantized(save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc4fda7-432d-4916-9ec7-30d8b2a2b3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2391924f-817d-4da5-89a0-dddc0e87eb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 769.83it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 26.48it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"models/quantized/llama-2-7b-2bit_8-group\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "model = LlamaHQQ.from_quantized(save_dir_or_hub=save_dir,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bff24e3-630d-4096-b059-517e461511ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = \"Write a recipe, how would you do an omlette.\"\n",
    "text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2af6dd-428f-449e-9b89-ed66d5429e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecab89c-8dcf-48a7-958b-fd87b589058f",
   "metadata": {},
   "source": [
    "### 2 bit -- group 32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d211d5-4c35-4111-95ff-04ea890fb1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Write a recipe, how would you do an omlette.\\nYou don't have to write a recipe, but I will give you the freedom to write as much or as little detail as you like. Let me know if you need further clarification.\",\n",
       " \"Write a recipe, how would you do an omlette.\\nYou don't have to write a recipe, but I will give you the freedom to write as much or as little detail as you like. Let me know if you want to add anything else.\",\n",
       " \"Write a recipe, how would you do an omlette.\\nYou don't have to write a recipe, but I will give you the freedom to write as much or as little detail as you like. Let me know if you need further clarification. 🙂\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_for_instruction(instruct, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "115a8062-8402-4701-b011-4eae855b899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [2:33:11<00:00, 13.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity 21.7443\n",
      "time 13.779  sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': 21.7443, 'prediction_time': 13.779}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantized model ppl\n",
    "eval_wikitext2(model, tokenizer, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4263b-3512-4b85-9e69-b7d5ffcc5638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "494b5b0c-1aeb-4b49-b222-6e706ab208b5",
   "metadata": {},
   "source": [
    "### 2 bit -- group 8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b2f8ba-a049-4af3-ad8a-c54429cfb5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = \"Write a recipe, how would you do an omlette.\"\n",
    "text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9bb36c8-9c41-48ac-bf62-d60e3ec1c905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Write a recipe, how would you do an omlette.\\nIf I were to write a recipe for an omelette, it would look something like this:\\nIngredients:\\n\\n* 2 eggs\\n* 1/4 cup diced vegetables (such as bell peppers, onions, and mushrooms)\\n* 1 tablespoon butter or oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Preheat a non-stick pan over medium heat.\\n2. In a bowl, beat the eggs and set aside.\\n3. Add the diced vegetables to the pan and cook until they are tender, about 5 minutes.\\n4. Pour the beaten eggs over the vegetables in the pan and cook until the eggs are set, about 2-3 minutes.\\n5. Use a spatula to gently fold the edges of the omelette towards the center, allowing the uncooked egg to flow to the edges.\\n6. Cook until the eggs are fully cooked and the omelette is golden brown, about 1-2 minutes more.\\n7. Slide the omelette onto a plate and serve hot.\\n\\nNote: You can customize this recipe by adding different ingredients such as cheese, herbs, or spices to suit your taste.',\n",
       " 'Write a recipe, how would you do an omlette.\\nIf I were to write a recipe for an omelette, it would look something like this:\\nIngredients:\\n\\n* 2 eggs\\n* 1/4 cup diced vegetables (such as bell peppers, onions, and mushrooms)\\n* 1 tablespoon butter or oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Preheat a non-stick pan over medium heat.\\n2. In a bowl, beat the eggs and set aside.\\n3. Add the diced vegetables to the pan and cook until they are tender, about 5 minutes.\\n4. Pour the beaten eggs over the vegetables in the pan and cook until the eggs are set, about 2-3 minutes.\\n5. Use a spatula to gently fold the edges of the omelette towards the center, allowing the uncooked egg to flow to the edges.\\n6. Cook until the eggs are fully cooked and the omelette is golden brown, about 1-2 minutes more.\\n7. Slide the omelette onto a plate and serve hot.\\n\\nNote: You can customize this recipe by adding different ingredients such as cheese, herbs, or spices to suit your taste.\\n\\nI hope this helps! Let me know if you have any questions.',\n",
       " 'Write a recipe, how would you do an omlette.\\nIf I were to write a recipe for an omelette, it would look something like this:\\nIngredients:\\n\\n* 2 eggs\\n* 1/4 cup diced vegetables (such as bell peppers, onions, and mushrooms)\\n* 1 tablespoon butter or oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Preheat a non-stick pan over medium heat.\\n2. In a bowl, beat the eggs and set aside.\\n3. Add the diced vegetables to the pan and cook until they are tender, about 5 minutes.\\n4. Pour the beaten eggs over the vegetables in the pan and cook until the eggs are set, about 2-3 minutes.\\n5. Use a spatula to gently fold the edges of the omelette towards the center, allowing the uncooked egg to flow to the edges.\\n6. Cook until the eggs are fully cooked and the omelette is golden brown, about 1-2 minutes more.\\n7. Slide the omelette onto a plate and serve hot.\\n\\nNote: You can customize this recipe by adding different ingredients such as cheese, herbs, or spices to suit your taste.\\n\\nI hope this helps! Let me know if you have any questions or need further clarification.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_for_instruction(instruct, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ac478e6-0402-4da6-8e1f-6f7470807410",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = \"Write a recipe, how would you do an omlette for the dark lord.\"\n",
    "text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab36f292-b2e5-4bf9-8293-4ddbb96fe526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Write a recipe, how would you do an omlette for the dark lord.\\nBy Dark Lord of Caerbannog, at 12:09 PM, February 07, 2008\\nHaha, I love it! Here's a recipe for an omelette fit for the Dark Lord of Caerbannog:\\n\\nIngredients:\\n\\n* 2 eggs\\n* 1/4 cup of dark, evil cheese (such as Gouda or Cheddar)\\n* 1/4 cup of minced dark mushrooms (for added darkness)\\n* 1/4 cup of chopped dark herbs (such as thyme or rosemary)\\n* 1/4 teaspoon of dark spices (such as cumin or coriander)\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Preheat your oven to 350 degrees Fahrenheit.\\n2. In a bowl, whisk together the eggs and dark, evil cheese until well combined.\\n3. Add the minced dark mushrooms, chopped dark herbs, and dark spices to the bowl and mix well.\\n4. Heat a non-stick pan over medium heat and add a small amount of oil.\\n5. Pour the egg mixture into the pan and cook until the edges start to set, about 2-3 minutes.\\n6. Use a spatula to carefully lift the edges of the omelette and allow the uncooked side to fall towards the bottom of the pan.\\n7. Once the omelette is almost set, add a sprinkle of dark, evil spices (such as cobwebs or bat wings) on top.\\n8. Cook for an additional 1-2 minutes, until the spices are fully incorporated and the omelette is cooked through.\\n9. Remove the omelette from the pan and place it on a dark, evil plate.\\n10. Serve with a side of dark, evil vegetables (such as turnips or rutabaga) and a glass of dark, evil ale (such as Black IPA or Dark Matter).\\n\\nEnjoy your meal, my dear! Bon appetit!\",\n",
       " \"Write a recipe, how would you do an omlette for the dark lord.\\nBy Dark Lord of Caerbannog, at 12:09 PM, February 07, 2008\\nHaha, I love it! Here's a recipe for an omelette fit for the Dark Lord of Caerbannog:\\n\\nIngredients:\\n\\n* 2 eggs\\n* 1/4 cup of dark, evil cheese (such as Gouda or Cheddar)\\n* 1/4 cup of minced dark mushrooms (for added darkness)\\n* 1/4 cup of chopped dark herbs (such as thyme or rosemary)\\n* 1/4 teaspoon of dark spices (such as cumin or coriander)\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Preheat your oven to 350 degrees Fahrenheit.\\n2. In a bowl, whisk together the eggs and dark, evil cheese until well combined.\\n3. Add the minced dark mushrooms, chopped dark herbs, and dark spices to the bowl and mix well.\\n4. Heat a non-stick pan over medium heat and add a small amount of oil.\\n5. Pour the egg mixture into the pan and cook until the edges start to set, about 2-3 minutes.\\n6. Use a spatula to carefully lift the edges of the omelette and allow the uncooked side to fall towards the bottom of the pan.\\n7. Once the omelette is almost set, add a sprinkle of dark, evil spices (such as cobwebs or bat wings) on top.\\n8. Cook for an additional 1-2 minutes, until the spices are fully incorporated and the omelette is cooked through.\\n9. Remove the omelette from the pan and place it on a dark, evil plate.\\n10. Serve with a side of dark, evil vegetables (such as turnips or rutabaga) and a glass of dark, evil ale (such as Black IPA or Dark Matter).\\n\\nEnjoy your delicious and evil omelette, my dear! Bon appetit!\",\n",
       " \"Write a recipe, how would you do an omlette for the dark lord.\\nBy Dark Lord of Caerbannog, at 12:09 PM, February 07, 2008\\nHaha, I love it! Here's a recipe for an omelette fit for the Dark Lord of Caerbannog:\\n\\nIngredients:\\n\\n* 2 eggs\\n* 1/4 cup of dark, evil cheese (such as Gouda or Cheddar)\\n* 1/4 cup of minced dark mushrooms (for added darkness)\\n* 1/4 cup of chopped dark herbs (such as thyme or rosemary)\\n* 1/4 teaspoon of dark spices (such as cumin or coriander)\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Preheat your oven to 350 degrees Fahrenheit.\\n2. In a bowl, whisk together the eggs and dark, evil cheese until well combined.\\n3. Add the minced dark mushrooms, chopped dark herbs, and dark spices to the bowl and mix well.\\n4. Heat a non-stick pan over medium heat and add a small amount of oil.\\n5. Pour the egg mixture into the pan and cook until the edges start to set, about 2-3 minutes.\\n6. Use a spatula to carefully lift the edges of the omelette and allow the uncooked side to fall towards the bottom of the pan.\\n7. Once the omelette is almost set, add a sprinkle of dark, evil spices (such as cobwebs or bat wings) on top.\\n8. Cook for an additional 1-2 minutes, until the spices are fully incorporated and the omelette is cooked through.\\n9. Remove the omelette from the pan and place it on a dark, evil plate.\\n10. Serve with a side of dark, evil vegetables (such as turnips or rutabaga) and a glass of dark, evil ale (such as Black IPA or Dark Mead).\\n\\nEnjoy your delicious and diabolical omelette, my dear! Bon appetit!\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_for_instruction(instruct, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f67c666-007b-4195-9741-c4482fca31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в принципе лама обучающем датасете Ламы были и русскоязычные примеры, так что можно проверить её способности\n",
    "instruct = \"Напиши рецепт карри в китайском стиле с морепродуктами.\"\n",
    "text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0acd7a36-16c5-46ee-b1ee-53f40227594c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Напиши рецепт карри в китайском стиле с морепродуктами.\\n\\nКарри в китайском стиле можно приготовить из различных морепродуктов, таких как кальмары, осья, креветки и т.д. Вот рецепт классического карри в китайском стиле с использованием морепродуктов:\\n\\nИнгредиенты:\\n\\n* 500 г морепродуктов (кальмары, осья, креветки и т.д.)\\n* 1/2 стакана растительного масла\\n* 1/4 стакана тоfu\\n* 1/4 стакана риса\\n* 1/4 стакана сушеного мяса (optional)\\n* 1 tablespoon soy sauce\\n* 1 tablespoon hoisin sauce\\n* 1 tablespoon rice vinegar\\n* 1 tablespoon honey\\n* 1 teaspoon sesame oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Clean and cut the seafood into bite-sized pieces.\\n2. Heat the vegetable oil in a wok or large skillet over medium-high heat.\\n3. Add the seafood and cook until it is lightly browned and crispy.\\n4. Remove the seafood from the wok and set aside.\\n5. In a small bowl, mix together the soy sauce, hoisin sauce, rice vinegar, honey, and sesame oil.\\n6. Add the mixture to the wok and stir to combine with the seafood.\\n7. Cook for 2-3 minutes or until the sauce has thickened slightly.\\n8. Season with salt and pepper to taste.\\n9. Serve immediately over rice or noodles.\\n\\nThis recipe is a simple and delicious way to prepare a traditional Chinese-style carri with seafood. You can adjust the amount of ingredients according to your preference and dietary needs. Enjoy!',\n",
       " 'Напиши рецепт карри в китайском стиле с морепродуктами.\\n\\nКарри в китайском стиле можно приготовить из различных морепродуктов, таких как кальмары, осья, креветки и т.д. Вот рецепт классического карри в китайском стиле с использованием морепродуктов:\\n\\nИнгредиенты:\\n\\n* 500 г морепродуктов (кальмары, осья, креветки и т.д.)\\n* 1/2 стакана растительного масла\\n* 1/4 стакана тоfu\\n* 1/4 стакана риса\\n* 1/4 стакана сушеного мяса (optional)\\n* 1 tablespoon soy sauce\\n* 1 tablespoon hoisin sauce\\n* 1 tablespoon rice vinegar\\n* 1 tablespoon honey\\n* 1 teaspoon sesame oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Clean and cut the seafood into bite-sized pieces.\\n2. Heat the vegetable oil in a wok or large skillet over medium-high heat.\\n3. Add the seafood and cook until it is lightly browned and crispy.\\n4. Remove the seafood from the wok and set aside.\\n5. In a small bowl, mix together the soy sauce, hoisin sauce, rice vinegar, honey, and sesame oil.\\n6. Add the mixture to the wok and stir to combine with the seafood.\\n7. Cook for 2-3 minutes or until the sauce has thickened slightly.\\n8. Season with salt and pepper to taste.\\n9. Serve immediately over rice or noodles.\\n\\nThis recipe is a simple and delicious way to prepare a traditional Chinese-style carrie with seafood. You can adjust the amount of ingredients according to your personal preference and dietary needs. Enjoy!',\n",
       " 'Напиши рецепт карри в китайском стиле с морепродуктами.\\n\\nКарри в китайском стиле можно приготовить из различных морепродуктов, таких как кальмары, осья, креветки и т.д. Вот рецепт классического карри в китайском стиле с использованием морепродуктов:\\n\\nИнгредиенты:\\n\\n* 500 г морепродуктов (кальмары, осья, креветки и т.д.)\\n* 1/2 стакана растительного масла\\n* 1/4 стакана тоfu\\n* 1/4 стакана риса\\n* 1/4 стакана сушеного мяса (optional)\\n* 1 tablespoon soy sauce\\n* 1 tablespoon hoisin sauce\\n* 1 tablespoon rice vinegar\\n* 1 tablespoon honey\\n* 1 teaspoon sesame oil\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. Clean and cut the seafood into bite-sized pieces.\\n2. Heat the vegetable oil in a wok or large skillet over medium-high heat.\\n3. Add the seafood and cook until it is lightly browned and crispy.\\n4. Remove the seafood from the wok and set aside.\\n5. In a small bowl, mix together the soy sauce, hoisin sauce, rice vinegar, honey, and sesame oil.\\n6. Add the mixture to the wok and stir to combine with the seafood.\\n7. Cook for 2-3 minutes or until the sauce has thickened slightly.\\n8. Season with salt and pepper to taste.\\n9. Serve immediately over rice or noodles.\\n\\nThis recipe is a simple and delicious way to prepare a traditional Chinese-style carri with seafood. You can adjust the amount of ingredients according to your personal preference and serving size. Enjoy!']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пока выглядит не очень -- в особенности с переходом на латиницу \n",
    "predict_for_instruction(instruct, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a89644af-b96c-453f-bd3d-70858568c486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667/667 [3:02:14<00:00, 16.39s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity 7.7676\n",
      "time 16.384  sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity': 7.7676, 'prediction_time': 16.384}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantized model ppl\n",
    "eval_wikitext2(model, tokenizer, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ac1c3b4-f50a-495e-8112-dd7ae941f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a6213a-6ff1-42df-9fec-84349c3d9d52",
   "metadata": {},
   "source": [
    "Not that bad -- 7.76 is alright for our task, let's stick to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910bf96f-f512-4421-98c2-defc08eabad4",
   "metadata": {},
   "source": [
    "### 2 bit, group 8 model + Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd45511-7a3b-49c6-a4a2-67acd1b43a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4bcc61-a462-42d8-9cf6-c614737a49a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 785.41it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 25.73it/s]\n"
     ]
    }
   ],
   "source": [
    "cache_path = './models'\n",
    "\n",
    "model_id  = \"meta-llama/Llama-2-7b-hf\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "save_dir = \"models/quantized/llama-2-7b-2bit_8-group\"\n",
    "model = LlamaHQQ.from_quantized(save_dir_or_hub=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c796569-484b-48e9-946f-a58bb00701d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 127.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dtype = torch.float32 \n",
    "base_lora_params = {'lora_type':'default', 'r': 10, 'lora_alpha':10, 'dropout':0.05, 'train_dtype':train_dtype}\n",
    "mlp_lora_params = {'lora_type': 'default', 'r': 6, 'lora_alpha':8, 'dropout':0.15, 'train_dtype':train_dtype}\n",
    "lora_params = {'self_attn.q_proj': base_lora_params,\n",
    "\t\t    'self_attn.k_proj': base_lora_params,\n",
    "\t\t    'self_attn.v_proj': base_lora_params,\n",
    "\t\t    'self_attn.o_proj': base_lora_params,\n",
    "\t\t    'mlp.gate_proj'   : mlp_lora_params,\n",
    "\t\t    'mlp.up_proj'     : mlp_lora_params,\n",
    "\t\t    'mlp.down_proj'   : mlp_lora_params}\n",
    "\n",
    "#Apply LoRA\n",
    "PeftUtils.add_lora(model, lora_params, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1528ad-6ef4-466f-ba2c-ddd37cd3284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19185664 || all params: 1900597248 || trainable%: 1.0094544764909603\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd42a7b-a4c5-4239-8928-1b474677fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import numpy as np \n",
    "import random\n",
    "from typing import Sequence\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(\"INFO\")\n",
    "\n",
    "tokenizer.pad_token     = tokenizer.eos_token \n",
    "tokenizer.padding_side  = \"right\" \n",
    "\n",
    "max_tokens = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcdd3795-cd5b-4bdd-993c-2dd0b8c9ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxenomirant\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/local/CCLP/peft/wandb/run-20240511_185524-1ytb876z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xenomirant/uncategorized/runs/1ytb876z' target=\"_blank\">2bit lora instruct</a></strong> to <a href='https://wandb.ai/xenomirant/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xenomirant/uncategorized' target=\"_blank\">https://wandb.ai/xenomirant/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xenomirant/uncategorized/runs/1ytb876z' target=\"_blank\">https://wandb.ai/xenomirant/uncategorized/runs/1ytb876z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/xenomirant/uncategorized/runs/1ytb876z?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79f6d0547bb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    name=\"2bit lora instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f942a89-682d-4d1a-ac25-5e383c831748",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Инструкция:\\n{instruction}\\n\\n### Контекст:\\n{input}\\n\\n### Ответ: \"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"### Инструкция:\\n{instruction}\\n\\n### Ответ: \"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4eda953-d7a9-4b62-b02b-a9dc48364030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_tokens,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ec877-6d36-4dec-8954-15b5afa550d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d14d72-2645-4acd-bc30-ef7bf01cc879",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    # cat targets with outputs\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    # set up labels for text2text\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    # change label ids whithin source length to ignore during loss computation\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054712fc-d6e9-4536-9725-26aff38d0469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b116d9d-3a31-4ce2-8065-a1bb31b4a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, data):\n",
    "        super().__init__()\n",
    "        logger.warning(\"Loading data...\")\n",
    "        list_data_dict = data\n",
    "\n",
    "        logger.warning(\"Formatting inputs...\")\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "        sources = [\n",
    "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "        logger.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[dict]) -> dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        # pad with IGNORE INDEX as well till the end\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        # return dict with attension mask for padded input values\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a0cb8d-27c3-474d-9f7b-7161b0fba7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap model to avoid accelerate issues \n",
    "class WrappedModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model.forward(*args, **kwargs)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d045870-4669-43b3-a235-8facd8921197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for IlyaGusev/ru_turbo_alpaca contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/IlyaGusev/ru_turbo_alpaca\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"IlyaGusev/ru_turbo_alpaca\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28707a3f-f55d-4c76-8159-6837cc266ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Formatting inputs...\n",
      "Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SupervisedDataset(tokenizer=tokenizer, data=data)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfade67a-ff40-4191-8e6b-a642568b3a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/.local/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "grad_acc   = 2\n",
    "logging_st = 20\n",
    "lr         = 3e-4 \n",
    "batch_size = 1\n",
    "n_epochs   = 2\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir='./models/2bit llama instruct/',\t\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=n_epochs,\n",
    "    remove_unused_columns=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=logging_st, \n",
    "    optim=\"adafactor\",\n",
    "    max_grad_norm=2.0,\n",
    "    save_steps=10000,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=WrappedModel(model),\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    max_seq_length=max_tokens,\n",
    "    packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e36272fe-b4cf-498c-92a1-934054cedb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='29822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5001/29822 23:19:53 < 115:50:44, 0.06 it/s, Epoch 0.34/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.564900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.748300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.622800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.634200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.631300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.516400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.763500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.465400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.563700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.824100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.528400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.637500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.422300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.585600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.657600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.548400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.817400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.837100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.498400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.525500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.655600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.553600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.798700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.488500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>1.319800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>1.914400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.648900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>1.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.630900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>1.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>1.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>1.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>1.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>1.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>1.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>1.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.479800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>1.548600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>1.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.559200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.710400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>1.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>1.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.787400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.578300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>1.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>1.596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>1.530700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.763500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>1.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>1.772200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>1.841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>1.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>1.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>1.459300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>1.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>1.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>1.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>1.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>1.564800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>1.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.477300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>1.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>1.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>1.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>1.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>1.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>1.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>1.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>1.550200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>1.591900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>1.660500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>1.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>1.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.543300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>1.632900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>1.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>1.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>1.803700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>1.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>1.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>1.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>1.889700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>1.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>1.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>1.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>1.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>1.779300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>1.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>1.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>1.547600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>1.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>1.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>1.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>1.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.575500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>1.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>1.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>1.461700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>1.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.801100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>1.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>1.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>1.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>1.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>1.748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>1.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>1.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>1.756700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>1.850300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>1.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>1.661900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>1.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>1.841300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>1.625400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>1.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>1.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>1.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>1.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>1.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>1.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>1.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>1.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>1.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>1.541800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/local/.local/lib/python3.10/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 233, in run\n",
      "    self._record_writer.write(data)\n",
      "  File \"/home/local/.local/lib/python3.10/site-packages/tensorboard/summary/writer/record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"/home/local/.local/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 766, in write\n",
      "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
      "  File \"/home/local/.local/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 160, in append\n",
      "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
      "  File \"/home/local/.local/lib/python3.10/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 164, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: b'./models/2bit llama instruct/runs/May11_18-56-03_512f8b197564/events.out.tfevents.1715453763.512f8b197564.66831.0'\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "with torch.autocast(\"cuda\"):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d66eb7-de28-440d-8e92-19a521558209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f0da8-1ad7-4e02-b7b5-f352501576cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98cfa626-eb69-4692-a887-7a102a7c99ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 6580.59it/s]\n"
     ]
    }
   ],
   "source": [
    "PeftUtils.save_lora_weights(model, filename=\"./models/lora/instruct_full\", base_class=LlamaHQQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0193bad4-003d-43c5-a6ba-76c02634fc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 943.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "#Convert lora weights to the same model dtype for faster inference\n",
    "PeftUtils.cast_lora_weights(model, dtype=torch.half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a942a936-05b1-40f3-8b58-80f5e3adc236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 6909.89it/s]\n"
     ]
    }
   ],
   "source": [
    "PeftUtils.save_lora_weights(model, filename=\"./models/lora/instruct_half\", base_class=LlamaHQQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ad779f0-4bd4-4895-bc45-86680efe7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b70aa-504b-4fa2-b5d4-90f8a6152b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13995dfc-6b91-49ef-a6d7-6411a81579cf",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce0aeb1d-82ba-4c38-bbb5-67c46cf95c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 756.25it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 26.33it/s]\n"
     ]
    }
   ],
   "source": [
    "cache_path = './models'\n",
    "\n",
    "model_id  = \"meta-llama/Llama-2-7b-hf\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "save_dir = \"models/quantized/llama-2-7b-2bit_8-group\"\n",
    "model = LlamaHQQ.from_quantized(save_dir_or_hub=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ad5661-5d53-4296-95f0-565611a86712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 123.58it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 768.95it/s]\n"
     ]
    }
   ],
   "source": [
    "PeftUtils.load_lora_weights(model, filename=\"./models/lora/instruct_full\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbb47977-4e44-4f25-855e-dcf9e95fb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = \"Write a recipe, how would you do an omlette.\"\n",
    "text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b8b668b-c484-438d-ac9c-5deeb3a57b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Write a recipe, how would you do an omlette.\\n',\n",
       " 'Write a recipe, how would you do an omlette.\\n',\n",
       " 'Write a recipe, how would you do an omlette.\\n']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_for_instruction(instruct, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f0e54a2-88c9-4971-a74f-78d30c301366",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct = \"Write a recipe, how would you do an omlette for the dark lord.\"\n",
    "text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e54d6a2-2bf6-4ea4-9366-722baf662dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Write a recipe, how would you do an omlette for the dark lord.\\n',\n",
       " 'Write a recipe, how would you do an omlette for the dark lord.\\n',\n",
       " 'Write a recipe, how would you do an omlette for the dark lord.\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_for_instruction(instruct, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8fe3bb9-0c91-40da-9352-3d7244d34e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в принципе лама обучающем датасете Ламы были и русскоязычные примеры, так что можно проверить её способности\n",
    "instruct = \"Напиши рецепт карри в китайском стиле с морепродуктами.\"\n",
    "text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eba79298-c540-4b4b-9b21-a93307b12993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Напиши рецепт карри в китайском стиле с морепродуктами.\\n\\n\\n\\n\\n',\n",
       " 'Напиши рецепт карри в китайском стиле с морепродуктами.\\n\\n\\n\\n\\n',\n",
       " 'Напиши рецепт карри в китайском стиле с морепродуктами.\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пока выглядит не очень -- в особенности с переходом на латиницу \n",
    "predict_for_instruction(instruct, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef6c2873-454b-425a-81c6-99a9ca183c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['### Инструкция:\\nНапиши рецепт карри в китайском стиле с морепродуктами.\\n\\n### Ответ: \\n\\n the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '### Инструкция:\\nНапиши рецепт карри в китайском стиле с морепродуктами.\\n\\n### Ответ: \\n\\n the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '### Инструкция:\\nНапиши рецепт карри в китайском стиле с морепродуктами.\\n\\n### Ответ: \\n\\n the\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пока выглядит не очень -- в особенности с переходом на латиницу \n",
    "predict_for_instruction(PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruct), text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ea7c8-62c3-4779-bdb7-0024a233eed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff371ae9-a0f1-4665-8e34-0c49fe342644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORA model ppl\n",
    "eval_wikitext2(model, tokenizer, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c249a76-d2e8-4234-a68f-0001ca53aaec",
   "metadata": {},
   "source": [
    "### Да, получилось плоховато -- модель скорее развалилась, чем доучилось на новую задачу. С другой стороны, мест, где что-то могло пойти не так тут довольно много. Во всяком случае это было довольно интересено."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3856452-f96d-4a07-89d2-fc3a62335f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f86a1322-b969-4a13-abf1-33b7b37f0480",
   "metadata": {},
   "source": [
    "### Playing with weight matrix visualization (not connected to the task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a797aab-7d21-4e66-a132-264441cccb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = model.model.layers[1].mlp.gate_proj.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa90cbcf-d687-4167-8ac9-d0eb2bb2f5b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1,  ..., 0, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 0, 0],\n",
       "        [1, 0, 1,  ..., 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0,  ..., 0, 1, 1],\n",
       "        [1, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 0, 1,  ..., 0, 0, 1]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_1bit_u8(model.model.layers[1].mlp.gate_proj.W_q).reshape(meta[\"shape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae7b83ca-aac8-45e0-8d7f-0e3522719e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_2bit_u8(W_q: torch.Tensor) -> torch.Tensor:  # uint8 > uint8/4\n",
    "    W_q = W_q.to(uint8)\n",
    "    _step = int(len(W_q) / 4)\n",
    "    print(_step)\n",
    "\n",
    "    return (\n",
    "        W_q[:_step] << 6\n",
    "        | W_q[_step : 2 * _step] << 4\n",
    "        | W_q[2 * _step : 3 * _step] << 2\n",
    "        | W_q[3 * _step :]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "239f7eba-71b6-4d33-86c3-5c017a050896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_1bit_u8(W_q: Tensor) -> Tensor:\n",
    "    W_q = W_q.to(uint8)\n",
    "    _step = int(len(W_q) / 8)\n",
    "\n",
    "    return (\n",
    "        W_q[:_step] << 7\n",
    "        | W_q[1 * _step : 2 * _step] << 6\n",
    "        | W_q[2 * _step : 3 * _step] << 5\n",
    "        | W_q[3 * _step : 4 * _step] << 4\n",
    "        | W_q[4 * _step : 5 * _step] << 3\n",
    "        | W_q[5 * _step : 6 * _step] << 2\n",
    "        | W_q[6 * _step : 7 * _step] << 1\n",
    "        | W_q[7 * _step : 8 * _step]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1e6b548-e8a4-4057-a222-6a51a48ad42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_1bit_u8(W_q: Tensor, dtype=uint8) -> Tensor:\n",
    "        _step = W_q.shape[0]\n",
    "        tmp = torch.empty([8 * _step, W_q.shape[1]], dtype=dtype, device=W_q.device)\n",
    "\n",
    "        tmp[0 * _step : 1 * _step] = (W_q & 0b10000000) >> 7\n",
    "        tmp[1 * _step : 2 * _step] = (W_q & 0b01000000) >> 6\n",
    "        tmp[2 * _step : 3 * _step] = (W_q & 0b00100000) >> 5\n",
    "        tmp[3 * _step : 4 * _step] = (W_q & 0b00010000) >> 4\n",
    "        tmp[4 * _step : 5 * _step] = (W_q & 0b00001000) >> 3\n",
    "        tmp[5 * _step : 6 * _step] = (W_q & 0b00000100) >> 2\n",
    "        tmp[6 * _step : 7 * _step] = (W_q & 0b00000010) >> 1\n",
    "        tmp[7 * _step : 8 * _step] = W_q & 0b00000001\n",
    "\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3650a017-639d-4c2a-9a20-9b796404a023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 262144])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.model.layers[0].self_attn.q_proj.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cbf74-5ad9-4c25-b760-544f798ec87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_batched(model, tokenizer, predictions, encodings=None, batch_size=1, add_start_token=True, device='cuda', max_length=None):\n",
    "    if tokenizer.pad_token is None and batch_size > 1:\n",
    "        existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "        # check that the model already has at least one special token defined\n",
    "        assert (len(existing_special_tokens) > 0), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "        # assign one of the special tokens to also be the pad token\n",
    "        tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "    if add_start_token and max_length:\n",
    "        # leave room for <BOS> token to be added:\n",
    "        assert (tokenizer.bos_token is not None), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "        max_tokenized_len = max_length - 1\n",
    "    else:\n",
    "        max_tokenized_len = max_length\n",
    "\n",
    "\n",
    "    if(encodings is None):\n",
    "        encodings = tokenizer(\n",
    "            predictions,\n",
    "            add_special_tokens=False,\n",
    "            padding=True,\n",
    "            truncation=True if max_tokenized_len else False,\n",
    "            max_length=max_tokenized_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True).to(device)\n",
    "\n",
    "    encoded_texts = encodings[\"input_ids\"]\n",
    "    attn_masks    = encodings[\"attention_mask\"]\n",
    "\n",
    "    # check that each input is long enough:\n",
    "    if add_start_token:\n",
    "        assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "    else:\n",
    "        assert torch.all(\n",
    "            torch.ge(attn_masks.sum(1), 2)\n",
    "        ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "    ppls = []\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    for start_index in tqdm(range(0, len(encoded_texts), batch_size)):\n",
    "        end_index     = min(start_index + batch_size, len(encoded_texts))\n",
    "        encoded_batch = encoded_texts[start_index:end_index]\n",
    "        attn_mask     = attn_masks[start_index:end_index]\n",
    "\n",
    "        if add_start_token:\n",
    "            bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_batch.size(dim=0)).to(device)\n",
    "            encoded_batch     = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "            attn_mask         = torch.cat([torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(device), attn_mask], dim=1)\n",
    "\n",
    "        labels = encoded_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "\n",
    "        shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "        perplexity_batch = torch.exp(\n",
    "            (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "            / shift_attention_mask_batch.sum(1))\n",
    "\n",
    "        ppls += perplexity_batch.tolist()\n",
    "\n",
    "    return np.mean(ppls)\n",
    "\n",
    "print('perplexity', compute_perplexity_batched(model=model, tokenizer=tokenizer, predictions=[s['text'] for s in dataset_val], batch_size=1, max_length=max_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
