{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be25c",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b6680619-c503-4192-9158-0d0e71c090d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, regex, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import textdistance\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cb2bef-83ae-42cf-a43c-becb891816f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2abf31e-addb-4fec-b429-916bae7c4f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5249181c-8bd3-4f13-b589-0cb875127ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa287612-46ca-4ad9-9f4c-3b9f2f5f41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = []\n",
    "total = 0\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    \n",
    "    \n",
    "    for pair in word_pairs:\n",
    "        if pair[0] != pair[1]:\n",
    "            mistakes.append(pair)\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e0a5f5-e2d0-4a21-b0be-6cfb35bfd1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля ошибок -  0.12886443221610805\n"
     ]
    }
   ],
   "source": [
    "print('Доля ошибок - ', len(mistakes)/total )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e6e45e-b611-4369-b1fc-8259d95357c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c98da6fa-8e53-41a7-a0b1-d839cce79b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(re.findall('\\w+', corpus.lower()))\n",
    "\n",
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f8e8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "    \n",
    "    similarities = Counter()\n",
    "    \n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(topn)\n",
    "\n",
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    \n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]\n",
    "\n",
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return np.log(vocab[word] / N)\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*8)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    #print(lookup)\n",
    "    \n",
    "    closest = get_closest_match_with_metric(text, lookup, topn * 3, metric=metric)\n",
    "    #print(closest)\n",
    "    \n",
    "    min_d = max(closest, key=itemgetter(1))[1]\n",
    "\n",
    "    max_p = -np.inf\n",
    "    \n",
    "    for word, dist in closest:\n",
    "        if P(word) > max_p:\n",
    "            target = (word, dist, P(word))\n",
    "            max_p = P(word)\n",
    "\n",
    "        if dist < min_d:\n",
    "            break\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c6b44-fef7-40e4-af28-a8a709394e50",
   "metadata": {},
   "source": [
    "Стоит заметить, что у подхода с CountVectorizer тоже есть некоторые недостатки - как минимум, слова с имеющимися в слове буквами в любом случае будут получать значительно более близкие косинусные расстояния, т.к. слова с отсутствующими буквами будут ортогональными как минимум в одной из размерностей. Таким образом смещая ожидаемые вероятности при финальном выборе - так, в примере ниже, чтобы получить слово выдра необходимо ровно столько же операций, как и для слово __сыра__, но оно уже не прошло, отфильтровавшись ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24bc9185-7e25-4281-9d1e-5c8b8e8f2f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['фарсы', 'сарды', 'сафры', 'садыр', 'фасады', 'арсыфа', 'рассады', 'сандры', 'русафы', 'сыра', 'арфы', 'сары', 'сфар', 'сафы', 'расф', 'рады', 'дыра', 'садр', 'дарс', 'дары', 'фарс', 'фары', 'сады', 'расы']\n",
      "[('сыра', 0.6666666666666667), ('садыр', 0.5), ('сандры', 0.5), ('сфар', 0.5), ('дыра', 0.5), ('садр', 0.5), ('сарды', 0.33333333333333337), ('сафры', 0.33333333333333337), ('сары', 0.33333333333333337)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('сыра', 0.6666666666666667, 3.1036147607142114e-06)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_hybrid_match(\"сфыдра\", X, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26b3ec-c4f3-42a5-a88f-c3dca5bdb6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ff5e4a0-f0a4-412b-aad6-f02a778e56ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 915/915 [04:01<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if predict_mistaken(pair[1], vocab):\n",
    "            pred = cashed.get(pair[1], get_closest_hybrid_match(pair[1], X, vec)[0][0])\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "        \n",
    "            \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d5a45-8a33-424c-897c-5bcaa6ac1049",
   "metadata": {},
   "source": [
    "Это предсказуемо не решило проблему с появлением новых ошибок, но могло сделать получающиеся в результате коррекции слова чуть более ожидаемыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de730a1b-b181-41b3-9b8f-1d9ce1184b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7928964482241121\n",
      "Mistakes not fixed: 0.0015527950310559005\n",
      "Mistakes made: 0.09004249454461927\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {correct/total}\")\n",
    "print(f\"Mistakes not fixed: {mistaken_fixed/total_mistaken}\")\n",
    "print(f\"Mistakes made: {correct_broken/total_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (7 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc14591-310a-4ed1-8cc9-ea74ec5a96d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "426b5aae-f490-4083-84d0-1afeb384ef86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "А еще, кажется, в силу наличия ударений в википедии, наша регулярка сработала слегка против нас, добавив ошибок в словарь. Попробуем пофиксить. Хотя в таком случае в нашем словаре одно и то же слово с ударением или без будут представлены разными входами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76e2a016-025b-4b44-8467-9cee5fffe111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'######Новостройка (Нижегородская область)############Новостро́йка — сельский посёлок в Дивеевском ра'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7bb89695-69e9-46eb-96d1-cb47f3f914b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'новостро'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', corpus.lower())[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cc59e-1ee2-4cb9-bbf9-27ab5fbe6a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b7a6f85-71d3-4660-9f98-de5d941bace3",
   "metadata": {},
   "source": [
    "### \"Эксперименты\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "26fe5f6f-0744-4384-8b04-ead0a92f6fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xd0\\xbe\\xcc\\x81'\n",
      "b'\\xd0\\xbe'\n"
     ]
    }
   ],
   "source": [
    "char = \"о́\"\n",
    "char2 = \"о\"\n",
    "print(char.encode())\n",
    "print(char2.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3eb49be1-e815-4c9f-904d-45d4cb80882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_t = b\"\\xcc\\x81\"\n",
    "\n",
    "regex = \"[\\w]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4509baf-3467-456a-a1cc-0abccef9524d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94399873-02e5-4f89-bb31-c597f9d4ac08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3f65288a-3099-4621-9d02-ef6cf1a0abc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'новостро́йка'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.findall(r\"[\\p{L}\\p{M}\\d'-]+\", corpus.lower())[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149710ea-54da-4890-a3a7-2d52d95ccc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dce0f153-9f18-4744-b056-9b787225bacd",
   "metadata": {},
   "source": [
    "### Новый словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "29dd698b-6355-4ad0-bb1d-92757dd662d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(regex.findall(r\"[\\p{L}\\p{M}\\d'-]+\", corpus.lower()))\n",
    "\n",
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3da78dc4-af49-49cd-8a4b-7f9ee8582473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 915/915 [03:57<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if predict_mistaken(pair[1], vocab):\n",
    "            pred = cashed.get(pair[1], get_closest_hybrid_match(pair[1], X, vec)[0][0])\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "        \n",
    "            \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6f462-adbc-4f2f-9957-5384ce945b78",
   "metadata": {},
   "source": [
    "Ну, чуть лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "12ee2507-637d-4f44-84dd-236a0762397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7987993996998499\n",
      "Mistakes not fixed: 0.0015527950310559005\n",
      "Mistakes made: 0.08326633742965431\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {correct/total}\")\n",
    "print(f\"Mistakes not fixed: {mistaken_fixed/total_mistaken}\")\n",
    "print(f\"Mistakes made: {correct_broken/total_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4638cc-9b4e-4561-ad0d-b29fdb7f87b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Он основан только на одной операции - удалении символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово  (!) \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2edbbd-336f-4732-b26e-7f18c0696719",
   "metadata": {},
   "source": [
    "Вообще алгоритм явно позволяет исправлять на нужные написания только случаи опечаток, что в общем случае верно далеко не всегда..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9e701e40-7f98-4940-8782-4c1a4af2d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_del_dict(vocab: Counter, deletions = 1):\n",
    "\n",
    "    del_dict = {}\n",
    "    \n",
    "    for word in vocab:\n",
    "        for j in range(deletions):\n",
    "            for i in range(len(word)):\n",
    "\n",
    "                del_dict[word[:i] + word[i+j:]] = word\n",
    "\n",
    "    return del_dict            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a4468834-1d13-4a70-af1c-f6e9f1b7829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "9b22db5a-93f6-49a5-909a-1aef0a7d37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deletions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0ec0bfc3-925c-44a6-a5c1-ac9bc86270bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = get_del_dict(vocab, deletions=deletions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e013acb-a2f1-4cae-8079-9d5aeb18efd5",
   "metadata": {},
   "source": [
    "Для удобства будем таки брать логарифм при расчете вероятностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "655bcfd7-ba85-440d-ba58-fb3a81561250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_symspell(word: str, del_dict: dict, vocab: dict, deletions: int, N: int):\n",
    "\n",
    "    def P(word, N=N):\n",
    "        return np.log(vocab[word] / N)\n",
    "\n",
    "    vars = []\n",
    "    finishers = []\n",
    "\n",
    "    for j in range(deletions):\n",
    "        for i in range(len(word)):\n",
    "            vars.append(word[:i] + word[i+j:])\n",
    "    \n",
    "    for str in vars:\n",
    "        if str in del_dict:\n",
    "            finishers.append(del_dict[str])\n",
    "    try:\n",
    "        return max(finishers, key=P)\n",
    "    except ValueError:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "dabc35e5-0d24-4b87-8b2e-ad7dadb54ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 915/915 [00:00<00:00, 21662.23it/s]\n"
     ]
    }
   ],
   "source": [
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if predict_mistaken(pair[1], vocab):\n",
    "            pred = cashed.get(pair[1], get_closest_symspell(pair[1], lookup_dict, vocab, deletions, N))\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "        \n",
    "            \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d79e8e-d49a-468e-b187-ed26097d2195",
   "metadata": {},
   "source": [
    "Между тем, результат заметно лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "41a40242-9c7b-4be1-a86e-90af50857d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8784392196098049\n",
      "Mistakes not fixed: 0.40683229813664595\n",
      "Mistakes made: 0.051797404387274606\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {correct/total}\")\n",
    "print(f\"Mistakes not fixed: {mistaken_fixed/total_mistaken}\")\n",
    "print(f\"Mistakes made: {correct_broken/total_correct}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
