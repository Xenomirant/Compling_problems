{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c678e33-7efc-47da-bd84-890daf4b5beb",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Имплементируйте алгоритм Леска (описание есть в семинаре) и оцените качество его работы на датасете `data/corpus_wsd_50k.txt`\n",
    "\n",
    "В качестве метрики близости вы должны попробовать два подхода:\n",
    "\n",
    "1) Jaccard score на множествах слов (определений и контекста)\n",
    "2) Cosine distance на эмбедингах sentence_transformers\n",
    "\n",
    "В качестве метрики используйте accuracy (% правильных ответов). Предсказывайте только многозначные слова в датасете\n",
    "\n",
    "Контекст вы можете определить самостоятельно (окно вокруг целевого слова или все предложение). Также можете поэкспериментировать с предобработкой для обоих методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035447f0-929b-4dfc-94d7-ec0579d5c6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "153a4330-4140-4507-a743-18264a8ae784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/corpus_wsd_50k.txt.zip\n",
      "  inflating: data/corpus_wsd_50k.txt  \n",
      "  inflating: data/__MACOSX/._corpus_wsd_50k.txt  \n"
     ]
    }
   ],
   "source": [
    "! unzip -d data data/corpus_wsd_50k.txt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c303877-5936-49b1-9485-9ab83e178d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\thow\tHow\n",
      "long%3:00:02::\tlong\tlong\n",
      "\thave\thas\n",
      "\tit\tit\n",
      "be%2:42:03::\tbe\tbeen\n",
      "\tsince\tsince\n",
      "\tyou\tyou\n",
      "review%2:31:00::\treview\treviewed\n",
      "\tthe\tthe\n",
      "objective%1:09:00::\tobjective\tobjectives\n",
      "\tof\tof\n",
      "\tyou\tyour\n",
      "benefit%1:21:00::\tbenefit\tbenefit\n",
      "\tand\tand\n",
      "service%1:04:07::\tservice\tservice\n"
     ]
    }
   ],
   "source": [
    "! head -n 15 data/corpus_wsd_50k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db595346-16e7-421c-a826-f3ccf8486554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Callable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5814a100-9ba8-4787-b0ac-73fa657a3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac7ca0c-4e26-4cbd-ae9a-35a9d9459bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2addecc-6306-4fd0-b834-3ab707c47f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeskAlgos():\n",
    "    '''\n",
    "    context_length: the number of tokens to count as context length\n",
    "    sample: sentences to take from corpus for accuracy evaluation\n",
    "    algorithm: algorithm to test - JaccardDistance or CosineDistance currently avaliable\n",
    "    '''\n",
    "\n",
    "    __corpus : List\n",
    "    context_length: int\n",
    "    check_accuracy: Callable\n",
    "    \n",
    "    def __init__(self, algorithm: str, context_length = 5, sample = 8000) -> None:\n",
    "\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        corpus_wsd = []\n",
    "        corpus = open('data/corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "        for sent in corpus:\n",
    "            corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])\n",
    "\n",
    "        assert sample <= len(corpus_wsd)\n",
    "        \n",
    "        self.__corpus = random.sample(corpus_wsd, sample)\n",
    "        \n",
    "        if algorithm == \"JaccardDistance\":\n",
    "\n",
    "            self.check_accuracy = self.__Jaccard_checker\n",
    "\n",
    "        elif algorithm == \"CosineDistance\":\n",
    "\n",
    "            self.check_accuracy = self.__Cosine_checker\n",
    "            self.__model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError(\"Specified algorithm is not implemented\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __Jaccard_checker(self, context_length: int = None) -> float:\n",
    "\n",
    "        if context_length is None:\n",
    "            context_length = self.context_length\n",
    "            \n",
    "        counter = 0\n",
    "        true_counter = 0\n",
    "\n",
    "        for sentences in tqdm(self.__corpus):\n",
    "\n",
    "            try:\n",
    "                sentence = [word[2] for word in sentences if re.match(r\"\\w+\", word[2])]\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            drop_i = 0\n",
    "            \n",
    "            for i in range(len(sentences)):\n",
    "                \n",
    "                if sentences[i][0] == \"\":\n",
    "                    drop_i += 1\n",
    "                    continue\n",
    "\n",
    "                counter += 1\n",
    "                lemma = wn.lemma_from_key(sentences[i][0])\n",
    "                definition = lemma.synset().definition()    \n",
    "\n",
    "                poss_definitions = [w.definition().split() for w in wn.synsets(lemma.name())]\n",
    "\n",
    "                for k in range(len(poss_definitions)):\n",
    "\n",
    "                    poss_definitions[k] = set([wn.morphy(word) for word in poss_definitions[k]])\n",
    "                                \n",
    "                context = set(self.__get_context(sentence, i-drop_i, self.context_length))\n",
    "                \n",
    "                for j in range(len(poss_definitions)):\n",
    "                    max_score = -1\n",
    "                    score = self.__JaccardDistance(poss_definitions[j], context)\n",
    "                    if score >= max_score:\n",
    "                        max_j = j\n",
    "\n",
    "                predicted_definition = wn.synsets(lemma.name())[max_j].definition()\n",
    "            \n",
    "                if predicted_definition == definition:\n",
    "                    true_counter += 1\n",
    "\n",
    "        return true_counter/counter\n",
    "            \n",
    "\n",
    "    def __Cosine_checker(self, context_length: int = None) -> float:\n",
    "\n",
    "        # алгоритм явно не оптимален, было бы лучше собирать вместе все definitions и все contexts\n",
    "        # для каждого предложения, и уже их отправлять в модель - было бы явно несколько быстрее в обработке, но придумал я это уже слишком поздно\n",
    "        \n",
    "        if context_length is None:\n",
    "            context_length = self.context_length\n",
    "\n",
    "        counter = 0\n",
    "        true_counter = 0\n",
    "\n",
    "        for sentences in tqdm(self.__corpus):\n",
    "\n",
    "            try:\n",
    "                sentence = [word[2] for word in sentences if re.match(r\"\\w+\", word[2])]\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "            drop_i = 0\n",
    "\n",
    "            for i in range(len(sentences)):\n",
    "                \n",
    "                if sentences[i][0] == \"\":\n",
    "                    drop_i += 1\n",
    "                    continue\n",
    "\n",
    "                counter += 1\n",
    "                lemma = wn.lemma_from_key(sentences[i][0])\n",
    "                definition = lemma.synset().definition()  \n",
    "\n",
    "                poss_definitions = [w.definition() for w in wn.synsets(lemma.name())]\n",
    "                                                \n",
    "                context = \" \".join(self.__get_context(sentence, i-drop_i, self.context_length))\n",
    "\n",
    "                dist = self.__CosineDistance(poss_definitions, context)\n",
    "\n",
    "                predicted_definition = poss_definitions[dist.argmax()]\n",
    "            \n",
    "                if predicted_definition == definition:\n",
    "                    true_counter += 1\n",
    "\n",
    "        return true_counter/counter\n",
    "\n",
    "    \n",
    "\n",
    "    def __JaccardDistance(self, definition: set, context: set) -> float:\n",
    "    \n",
    "        dist = len(definition & context)/ len(definition | context)\n",
    "\n",
    "        return dist\n",
    "\n",
    "\n",
    "    def __CosineDistance(self, definitions: List[str], context: str) -> List[float]:\n",
    "\n",
    "\n",
    "        definitions_emb = [self.__model.encode(definition, \n",
    "                                              device = \"cpu\") for definition in definitions]\n",
    "        context_emb = self.__model.encode(context, \n",
    "                                          device = \"cpu\")\n",
    "\n",
    "        return cosine_distances(context_emb.reshape(1, -1), definitions_emb).ravel()\n",
    "\n",
    "    def __get_context(self, lst: List, position: int, n: int) -> List:\n",
    "\n",
    "        assert position < len(lst), \"position out of range\"\n",
    "    \n",
    "        start = max(0, position - n)\n",
    "        end = min(len(lst), position + n + 1)\n",
    "        lst = lst[start:position] + lst[position+1:end]\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a498a-ba96-46c0-bcce-aba85d0d2560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf86746-9b97-4d91-975c-6c404858aa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 8000/8000 [2:14:51<00:00,  1.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2049902024820379"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lesk = LeskAlgos(algorithm=\"CosineDistance\")\n",
    "\n",
    "Lesk.check_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "227dc937-c689-4773-a153-1556362e276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 8000/8000 [00:19<00:00, 400.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15414293112377006"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lesk2 = LeskAlgos(algorithm = \"JaccardDistance\")\n",
    "\n",
    "Lesk2.check_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6678e9-f44a-4ce0-9e2a-4c570c6dfcff",
   "metadata": {},
   "source": [
    "### На удивление не слишком существенная прибавка по точности, при гигантском увеличении времени и стоимости расчетов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efde9a-af0b-4c94-bfd0-249e7054562f",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "Попробуйте разные алгоритмы кластеризации на датасете - `https://github.com/nlpub/russe-wsi-kit/blob/initial/data/main/wiki-wiki/train.csv`\n",
    "\n",
    "Используйте код из семинара как основу. Используйте ARI как метрику качества.\n",
    "\n",
    "Попробуйте все 4 алгоритма кластеризации, про которые говорилось на семинаре. Для каждого из алгоритмов попробуйте настраивать гиперпараметры (посмотрите их в документации). Прогоните как минимум 5 экспериментов (не обязательно успешных) с разными параметрами на каждый алгоритме кластеризации и оцените: качество кластеризации, скорость работы, интуитивность параметров.\n",
    "\n",
    "Помимо этого также выберите 1 дополнительный алгоритм кластеризации отсюда - https://scikit-learn.org/stable/modules/clustering.html , опишите своими словами принцип его работы  и проделайте аналогичные эксперименты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d59bef3e-1af7-4ce2-b43a-dfef282050f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2023-10-09 15:56:18--  https://raw.githubusercontent.com/nlpub/russe-wsi-kit/initial/data/main/wiki-wiki/train.csv\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 365614 (357K) [text/plain]\n",
      "Saving to: ‘data/train.csv’\n",
      "\n",
      "data/train.csv      100%[===================>] 357.04K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-10-09 15:56:18 (3.00 MB/s) - ‘data/train.csv’ saved [365614/365614]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O data/train.csv https://raw.githubusercontent.com/nlpub/russe-wsi-kit/initial/data/main/wiki-wiki/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "462e1d1d-4436-40a7-a9d9-4b8aab553ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "context_id\tword\tgold_sense_id\tpredict_sense_id\tpositions\tcontext\n",
      "1\tзамок\t1\t\t0-5, 339-344\tзамок владимира мономаха в любече . многочисленные укрепленные монастыри также не являлись замками как таковыми — это были крепости . ранние европейские замки строились преимущественно из дерева они опоясывались деревянной оградой — палисадом уже тогда вокруг замков стали появляться рвы . примером такого замка может служить вышгородский замок киевских князей . каменное замковое строительство распространилось в западной и центральной европе лишь к xii веку . главной частью средневекового замка являлась центральная башня — донжон , выполнявшая функции цитадели . помимо своих оборонительных функций , донжон являлся непосредственным жилищем феодала . также в главной башне\n",
      "2\tзамок\t1\t\t11-16, 17-22, 188-193\tшильонский замок замок шильйон ( ) , известный в русскоязычной литературе как шильо́нский за́мок , расположен на швейцарской ривьере , у кромки женевского озера , в  км от города монтре . замок представляет собой комплекс из элементов разного времени постройки .\n",
      "3\tзамок\t1\t\t299-304\tпроведения архитектурно - археологических работ эстонским реставрационным управлением под руководством архитектора х .  и .  потти , искусствоведа е .  а .  кальюнди и при научной консультации доктора исторических наук п .  а .  рапопорта . с года музей называется государственным музеем выборгский замок .\n",
      "4\tзамок\t1\t\t111-116\tтопи с . , л . белокуров легенда о завещании мавра с . , н . юсупов день рождения с . , р . янушкевич янтарный замок с . .\n"
     ]
    }
   ],
   "source": [
    "! head -n 5 data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd85c06c-c059-4b94-862b-a3437bb9b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8126cb3-3874-4918-b14f-441165425f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/train.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd466e9-93ea-42c6-a27d-e78ee93313c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 439 entries, 0 to 438\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   context_id        439 non-null    int64  \n",
      " 1   word              439 non-null    object \n",
      " 2   gold_sense_id     439 non-null    int64  \n",
      " 3   predict_sense_id  0 non-null      float64\n",
      " 4   positions         439 non-null    object \n",
      " 5   context           439 non-null    object \n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 20.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddd9c0a2-f5f4-4c98-98a9-275ea472e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"predict_sense_id\", \"context_id\", \"positions\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7440f98-e22b-472e-a06f-8095d94608b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gold_sense_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee447674-fe90-49c5-8730-7848a8f44111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(\"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb781b96-7b57-48d4-aacc-acd5e9e58c74",
   "metadata": {},
   "source": [
    "### Clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86b9a003-aa33-44b4-abd0-e4dc4f416b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation, AgglomerativeClustering, Birch\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8156c315-08b1-4d24-9d2d-9a11e466a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01ab5d1-d8c9-48f5-99ea-9293d0e8e400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 17:01:43.271720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cpu\")\n",
    "embed = model.encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be11b33-e3d7-4e3b-82d9-353264b72c51",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b954a572-edb0-4975-b0c5-dfe021ddc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI = defaultdict(list)\n",
    "\n",
    "for key, _ in df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    params = {\"n_clusters\": [3, 5, 2]}\n",
    "\n",
    "    for param in params[\"n_clusters\"]:\n",
    "        cluster = KMeans(n_clusters=param, n_init = \"auto\")\n",
    "\n",
    "    \n",
    "        cluster.fit(X)\n",
    "        labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "        ARI[param].append(adjusted_rand_score(df.get_group(key)['gold_sense_id'], labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a83365e-a691-48b0-b639-dbdfe62d0c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for n_clusters = 3 is 0.06771171879804742\n",
      "\n",
      "\n",
      "Average score for n_clusters = 5 is 0.05520611923747333\n",
      "\n",
      "\n",
      "Average score for n_clusters = 2 is 0.027886782845293015\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in ARI.items():\n",
    "    print(f\"Average score for n_clusters = {key} is {np.mean(value)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cad7c1-c179-438b-9c1e-2ec2aaf1ff8b",
   "metadata": {},
   "source": [
    "### AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c69fc8bd-d144-4783-a1fb-75324d18f10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = defaultdict(list)\n",
    "\n",
    "for key, _ in df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    params = {\"damping\": np.arange(0.5, 0.99, 0.15)}\n",
    "    # выбираем один из алгоритмов\n",
    "    for param in params[\"damping\"]:\n",
    "        cluster = AffinityPropagation(damping=param)\n",
    "    #cluster = DBSCAN(min_samples=1, eps=0.1)\n",
    "    \n",
    "        cluster.fit(X)\n",
    "        labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "        ARI[param].append(adjusted_rand_score(df.get_group(key)['gold_sense_id'], labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a19a3b66-31e2-4fbe-b8ed-a5b60284aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for damping = 0.5 is 0.04270504600809172\n",
      "\n",
      "\n",
      "Average score for damping = 0.65 is 0.042363774919161074\n",
      "\n",
      "\n",
      "Average score for damping = 0.8 is 0.04154515818974152\n",
      "\n",
      "\n",
      "Average score for damping = 0.9500000000000001 is 0.04916074877739414\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in ARI.items():\n",
    "    print(f\"Average score for damping = {key} is {np.mean(value)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4a3ad-f0f8-4af4-b09b-5bc5bddab298",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f36071e-96a6-47a6-8aae-e1e3aac61446",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI = defaultdict(list)\n",
    "\n",
    "for key, _ in df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    params = {\"min_samples\": np.arange(1, 5, 1)}\n",
    "    \n",
    "    for param in params[\"min_samples\"]:\n",
    "        cluster = DBSCAN(min_samples=param, eps=0.1)\n",
    "        cluster.fit(X)\n",
    "        labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "        ARI[param].append(adjusted_rand_score(df.get_group(key)['gold_sense_id'], labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b70114e6-5c63-4022-9f5d-02a7f0377e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for min_samples = 1 is 0.001053019960000099\n",
      "\n",
      "\n",
      "Average score for min_samples = 2 is 0.015472784737676827\n",
      "\n",
      "\n",
      "Average score for min_samples = 3 is -0.0021290615824144776\n",
      "\n",
      "\n",
      "Average score for min_samples = 4 is -0.00867271551782085\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in ARI.items():\n",
    "    print(f\"Average score for min_samples = {key} is {np.mean(value)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca3cb7-eb6e-4e62-ab7d-7ae01245b9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13b436be-3583-4cdc-a1a2-0020e6ee53c3",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b42e87-c5e0-4ea2-a5f7-1b9a000b5390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e4c10832-36d1-4808-86ce-3a2e7fe1dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI = defaultdict(list)\n",
    "\n",
    "for key, _ in df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    params = {\"metric\": ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']}\n",
    "    \n",
    "    for param in params[\"metric\"]:\n",
    "        cluster = AgglomerativeClustering(metric=param, linkage=\"average\")\n",
    "        cluster.fit(X)\n",
    "        labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "        ARI[param].append(adjusted_rand_score(df.get_group(key)['gold_sense_id'], labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01c89de9-67a9-4ba8-a16d-0edfe933ef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for metric = euclidean is 0.0030178341081673436\n",
      "\n",
      "\n",
      "Average score for metric = l1 is 0.0030178341081673436\n",
      "\n",
      "\n",
      "Average score for metric = l2 is 0.0030178341081673436\n",
      "\n",
      "\n",
      "Average score for metric = manhattan is 0.0030178341081673436\n",
      "\n",
      "\n",
      "Average score for metric = cosine is 0.0030178341081673436\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in ARI.items():\n",
    "    print(f\"Average score for metric = {key} is {np.mean(value)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837272c-fc17-46df-b150-26aea11f4f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c370eed7-ca16-4f7c-af35-7d84f908b570",
   "metadata": {},
   "source": [
    "### Birch (appeard to be really non-optimal for the task due to the very nature of the algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2eb55965-85d4-4163-b802-acec18826213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (2) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n",
      "/home/xenos/Documents/masters/venv/lib/python3.11/site-packages/sklearn/cluster/_birch.py:725: ConvergenceWarning: Number of subclusters found (1) by BIRCH is less than (3). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ARI = defaultdict(list)\n",
    "\n",
    "for key, _ in df:\n",
    "    # вытаскиваем контексты\n",
    "    texts = df.get_group(key)['context'].values\n",
    "\n",
    "    # создаем пустую матрицу для векторов \n",
    "    X = np.zeros((len(texts), 768))\n",
    "\n",
    "    # переводим тексты в векторы и кладем в матрицу\n",
    "    for i, text in enumerate(texts):\n",
    "        X[i] = embed(text)\n",
    "\n",
    "    params = {\"threshold\": np.arange(0.5, 1, 0.1)}\n",
    "    \n",
    "    for param in params[\"threshold\"]:\n",
    "        cluster = Birch(threshold = param)\n",
    "        cluster.fit(X)\n",
    "        labels = np.array(cluster.labels_)+1 \n",
    "\n",
    "    # расчитываем метрику для отдельного слова\n",
    "        ARI[param].append(adjusted_rand_score(df.get_group(key)['gold_sense_id'], labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9162f1d-dc7e-43da-9ca1-6d61971a3af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for threshold = 0.5 is 0.018957481003908684\n",
      "\n",
      "\n",
      "Average score for threshold = 0.6 is 0.0\n",
      "\n",
      "\n",
      "Average score for threshold = 0.7 is 0.0\n",
      "\n",
      "\n",
      "Average score for threshold = 0.7999999999999999 is 0.0\n",
      "\n",
      "\n",
      "Average score for threshold = 0.8999999999999999 is 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in ARI.items():\n",
    "    print(f\"Average score for threshold = {key} is {np.mean(value)}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
