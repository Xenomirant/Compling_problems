{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3a8695-b528-4207-ab47-0f884b2f6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2ch_corpus.txt\", \"r\") as f:\n",
    "    dvach = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84bb021-4dd0-497d-87d6-2d35fb469f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143a073-a09f-47ce-a1b0-1a50643743c9",
   "metadata": {},
   "source": [
    "Длина корпуса в предложениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd187d20-8070-4e94-8dcb-167f8aefcadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170565"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokenize(dvach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3d2d18-8d79-4872-b1f0-7bdea67821b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = sent_tokenize(dvach)[:170500], sent_tokenize(dvach)[170500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3216659d-1a8e-45eb-8f4c-9a62bea425e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3812f-5257-450f-afa7-9d4585c9d9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97ac0ea-e9c9-46f4-81b6-d4f6a61b4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=3):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def n_markov_assumption_model(text, ngramms = 3):\n",
    "\n",
    "    text = [['<start>']*(ngramms-1) + normalize(sent) + ['<end>'] for sent in text]\n",
    "\n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    trigrams = Counter()\n",
    "\n",
    "    for sent in text:\n",
    "        unigrams.update(sent)\n",
    "        bigrams.update(ngrammer(sent, ngramms-1))\n",
    "        trigrams.update(ngrammer(sent, ngramms))\n",
    "\n",
    "    return unigrams, bigrams, trigrams\n",
    "\n",
    "\n",
    "def compute_joint_proba_markov_assumption(text, unigram_counts, bigram_counts, trigram_counts, ngramms = 3):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for ngram in ngrammer(['<start>']*(ngramms-1) + tokens + ['<end>'], ngramms):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigramm = \" \".join([word1, word2])\n",
    "        if bigramm in bigram_counts and ngram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[ngram]/bigram_counts[bigramm])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03c434-5333-42e2-93d2-498d1f603266",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94976600-ada4-4b6e-b5c8-3e2f3be15d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams, trigrams = n_markov_assumption_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ade7d9-4d70-4426-a2dc-e8b17116670d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8070a965-5274-4e11-ac25-f97304279b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-22.170201969452712, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Безграмотное быдло с дубляжом, войсовером, порнографией и котиками '\n",
    "\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams, bigrams, trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba68362-85fe-4bca-ba3e-0e8bcbb3ce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the model is 38756182.74456518\n"
     ]
    }
   ],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)\n",
    "\n",
    "\n",
    "ppl = []\n",
    "\n",
    "for text in test:\n",
    "\n",
    "    ppl.append(perplexity(*compute_joint_proba_markov_assumption(text, unigrams, bigrams, trigrams)))\n",
    "\n",
    "print(f\"Perplexity of the model is {np.array(ppl).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345beb29-f5f7-468e-8d7f-9ce4a4b89496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af82dce-aab7-4906-a9e1-4c21f045c4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebad40e4-ff06-4c1d-9757-d1f9c16c7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a01e0-e824-43b7-be6b-38da0010f709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec67b0a-5ffc-4e9a-9e73-48ea292ee1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams), \n",
    "                         len(unigrams)))\n",
    "\n",
    "id2word_unigrams = list(unigrams)\n",
    "id2word_bigrams = list(bigrams)\n",
    "word2id_unigrams = {word:i for i, word in enumerate(id2word_unigrams)}\n",
    "word2id_bigrams = {word:i for i, word in enumerate(id2word_bigrams)}\n",
    "\n",
    "for ngram in trigrams:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigramm = \" \".join([word1, word2])\n",
    "    \n",
    "    matrix_dvach[word2id_bigrams[bigramm], word2id_unigrams[word3]] =  (trigrams[ngram]/\n",
    "                                                                     bigrams[bigramm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db66d40-58a7-4a4f-a06e-094853a252e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f553ddb1-cdcc-4e8d-99a4-0982fbcfa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word_bigrams, word2id_bigrams, id2word_unigrams, word2id_unigrams, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = word2id_bigrams[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "\n",
    "        chosen_word = id2word_unigrams[chosen]\n",
    "        \n",
    "        text.append(chosen_word)\n",
    "\n",
    "        last_bigramm = id2word_bigrams[current_idx]\n",
    "\n",
    "        word1, word2 = last_bigramm.split()\n",
    "\n",
    "        chosen = word2id_bigrams[\" \".join([word2, chosen_word])]\n",
    "        \n",
    "        if chosen_word == '<end>':\n",
    "            chosen = word2id_bigrams['<start> <start>']\n",
    "        \n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44897519-21a8-4709-8e9f-2e3fa8e411dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "дабы сохранить все что ты горишь дома у каждого второго вижу моллеманькой либо миллисентом ебучим \n",
      " бюджет опять таки за счет того что синдзи превратил весь мир \n",
      " садитесь в пативен картина называется максим тесак \n",
      " на разные фракции \n",
      " спасибо бро поддержал \n",
      " podrasti suka bezmozglaya \n",
      " пояснение да есть такой реальный персонаж кто здесь недавно примерно так же постройки и арефакты фактически такие же отшибленные и будут выебываться как утерли нос американцам сукаааааа \n",
      " воспитала мать и дочь \n",
      " популярные сюжеты в кино-книгах одни и никто ничего не растет или растет медленней ввп то их поведение никак не\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_bigrams, word2id_bigrams, id2word_unigrams, word2id_unigrams).replace(\"<end>\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216e98f-4a29-48ae-9340-762cbeb4b72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c426746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем класс чтобы хранить каждый из лучей\n",
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af4cd2-8375-4d59-b8df-ec5edf5e8da5",
   "metadata": {},
   "source": [
    "##### Здесь, правда, теперь наоборот получается уж слишком детерминированное решение в любом случае"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6ebcac20-28d9-40eb-99f5-757cf4ce6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_with_beam_search(matrix, id2word_bigramm, word2id_bigramm, id2word_unigramm, word2id_unigramm, n=50, max_beams=7, start='<start> <start>'):\n",
    "    # изначально у нас один луч с заданным началом (start по дефолту)\n",
    "    initial_node = Beam(sequence=start.split(), score=np.log1p(0))\n",
    "    beams = [initial_node]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # делаем n шагов генерации\n",
    "        new_beams = []\n",
    "        # на каждом шаге продолжаем каждый из имеющихся лучей\n",
    "        for beam in beams:\n",
    "            # лучи которые уже закончены не продолжаем (но и не удаляем)\n",
    "            if beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "            # наша языковая модель предсказывает на основе предыдущего слова\n",
    "            # достанем его из beam.sequence\n",
    "            \n",
    "            last_id = word2id_bigramm[\" \".join(beam.sequence[-2:])]\n",
    "            \n",
    "            # посмотрим вероятности продолжений для предыдущего слова\n",
    "            probas = matrix[last_id].toarray()[0]\n",
    "            \n",
    "            # возьмем топ самых вероятных продолжений\n",
    "            top_idxs = probas.argsort()[:-(max_beams+1):-1]\n",
    "            for top_id in top_idxs:\n",
    "                # иногда вероятности будут нулевые, такое не добавляем\n",
    "                if not probas[top_id]:\n",
    "                    break\n",
    "                \n",
    "                # создадим новый луч на основе текущего и варианта продолжения\n",
    "                new_sequence = beam.sequence + [id2word_unigramm[top_id]]\n",
    "                # скор каждого луча это произведение вероятностей (или сумма логарифмов)\n",
    "                new_score = beam.score + np.log1p(probas[top_id])\n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        # отсортируем лучи по скору и возьмем только топ max_beams\n",
    "        beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
    "\n",
    "    \n",
    "    # в конце возвращаем случайный луч, чтобы алгоритм хотя бы на кототких последовательностях был чуть менее детерминированным\n",
    "    best_sequence = np.random.choice(beams).sequence\n",
    "\n",
    "    \n",
    "    return ' '.join(best_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0e97ec7e-48ff-458a-853b-6a0e6cd8d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ну и конечно же там друг возле друга близков случае чего кричат дурным голосом в рацию о подмоге чтобы прибежал мужик и по сей день считаю кровавые деньги лучшей частью а сан хорошую пушку со жмура теперь и кинамана побуждают к сотрудничеству первыми открывать огонь после того как я понял\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_dvach, id2word_bigrams, word2id_bigrams, id2word_unigrams, word2id_unigrams).replace(\"<start> <start>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95395052-b4b0-4490-b08a-267919ff288a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cf344575-6e3a-4f5b-b11d-d7db30d5b3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чего только стоит слияние руби и джаву\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_dvach, id2word_bigrams, word2id_bigrams, id2word_unigrams, word2id_unigrams, start = \"чего только\", n=5).replace(\"<start> <start>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1517821-7eb1-48c1-b43f-80b67628ee4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0325cf4b-9d17-47e6-81de-b96e091addc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чего только стоит js 100 часов лол\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_dvach, id2word_bigrams, word2id_bigrams, id2word_unigrams, word2id_unigrams, start = \"чего только\", n=5).replace(\"<start> <start>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe1243-6f0f-4edb-b1e4-1c5f79c28daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0f395447-ed4d-4b27-9202-be113a1dc96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "когда он пришел в приемную партии « единая россия партия дела <end>\n"
     ]
    }
   ],
   "source": [
    "print(generate_with_beam_search(matrix_dvach, id2word_bigrams, word2id_bigrams, id2word_unigrams, word2id_unigrams, start = \"когда он пришел\", n=10).replace(\"<start> <start>\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
